<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>When Patients Use AI Too | AI 101</title>
  <link rel="stylesheet" href="styles.css">
  <script src="https://unpkg.com/lucide@latest"></script>
</head>
<body>

  <nav class="nav">
    <div class="nav-inner">
      <a href="index.html" class="nav-brand">
        <span class="nav-badge">AI 101</span>
        <span class="nav-title">A Self-Paced Guide to AI in Medicine</span>
      </a>
      <button class="nav-toggle" aria-label="Toggle menu">
        <i data-lucide="menu"></i>
      </button>
      <div class="nav-links">
        <a href="index.html" class="nav-link">Topics</a>
        <a href="about.html" class="nav-link">About</a>
      </div>
    </div>
  </nav>

  <main class="main">
    <article class="content content-wide">

      <header class="unit-header">
        <span class="unit-label phase-2">USING AI</span>
        <h1 class="unit-title">When Patients Bring AI to the Exam Room</h1>
        <p class="unit-subtitle">
          Understanding, partnering with, and guiding patient use of AI health tools.
        </p>
        <div class="unit-meta">
          <span class="unit-meta-item">
            <i data-lucide="clock"></i>
            ~35 min read
          </span>
          <span class="unit-meta-item">
            <i data-lucide="users"></i>
            Patient communication
          </span>
        </div>
      </header>

      <div class="callout callout-question">
        <div class="callout-title">Core Question</div>
        <p class="mb-0">
          How can you transform patient AI use from a clinical challenge into an opportunity
          for deeper engagement and better outcomes?
        </p>
      </div>

      <!-- Introduction -->
      <h2>Introduction: The New Reality</h2>

      <p>
        Your patient walks in with a printout from ChatGPT suggesting she might have lupus. Another asks
        why you didn't order the MRI that "the AI" recommended. A parent wants to know if the treatment
        plan you've outlined matches what Google's Gemini told them about their child's ear infection.
        These scenarios are no longer hypothetical—they're becoming routine.
      </p>

      <p>
        According to a 2024 KFF poll, approximately 17% of U.S. adults now use AI chatbots at least monthly
        for health information and advice, with that figure climbing to 25% among adults under 30. A
        cross-sectional study published in the <em>Journal of Medical Internet Research</em> found that
        21.5% of respondents had used ChatGPT specifically for online health information. The Elsevier
        Clinician of the Future 2025 survey reveals a striking mismatch: while patients increasingly turn
        to AI tools, only 16% of clinicians report using AI in direct decision-making, and just 30% feel
        they've received adequate training.
      </p>

      <p>
        This topic examines the emerging landscape of patient AI use, how it manifests in clinical
        encounters, and—most importantly—how clinicians can transform what might feel like a challenge
        into an opportunity for deeper engagement and better outcomes.
      </p>

      <hr>

      <!-- Part 1 -->
      <h2>Part 1: Understanding How Patients Use AI</h2>

      <h3>Who Is Using AI for Health Information?</h3>

      <p>
        The demographic profile of health AI users challenges some assumptions. Research from ResearchMatch
        reveals that ChatGPT users for health information tend to be younger (average age 32.8 versus 39.1
        for non-users), but—perhaps surprisingly—have <em>lower</em> rates of advanced degree attainment
        (49.9% versus 67% holding a bachelor's degree or higher). These users also show greater utilization
        of transient healthcare settings like emergency departments and urgent care.
      </p>

      <p>
        This pattern suggests that AI health tools may be filling gaps where traditional primary care access
        falls short. Patients who lack a consistent primary care relationship, face longer wait times, or
        experience cost barriers may turn to AI as a more accessible—if imperfect—alternative.
      </p>

      <h3>What Are Patients Asking AI?</h3>

      <p>Patient AI queries cluster into several distinct categories:</p>

      <ul>
        <li><strong>Symptom interpretation and self-diagnosis:</strong> "I have a headache behind my eyes and a stiff neck—what could this be?"</li>
        <li><strong>Lab result explanation:</strong> Uploading MyChart screenshots and asking what their numbers mean</li>
        <li><strong>Medication questions:</strong> Side effects, interactions, whether a prescribed medication is "really necessary"</li>
        <li><strong>Treatment validation:</strong> Checking whether their doctor's recommendation aligns with what the AI suggests</li>
        <li><strong>Understanding diagnoses:</strong> "What is rheumatoid arthritis and what should I expect?"</li>
        <li><strong>Preparing for appointments:</strong> Generating questions to ask, understanding what tests might be ordered</li>
      </ul>

      <p>
        A study examining AI chatbot use for perioperative health information found four key themes: patients
        prefer AI chatbots over traditional search engines for their conversational interface, they appreciate
        the improved accessibility and perceived quality of information, they sometimes prefer AI interaction
        over human interaction for sensitive questions, and they recognize that effective prompting skills
        matter for getting useful responses.
      </p>

      <h3>Why Patients Choose AI Over Traditional Sources</h3>

      <p>
        The appeal is understandable when you consider the patient's perspective. As one patient told the
        <em>New York Times</em>: "ChatGPT has all day for me—it never rushes me out of the chat." This
        reflects a fundamental tension in modern healthcare: the average primary care visit lasts about
        18 minutes, with much of that time consumed by documentation and administrative tasks. AI offers
        unlimited time, immediate availability, and—crucially—no judgment.
      </p>

      <p>Research on patient motivations identifies several drivers:</p>

      <ul>
        <li><strong>Accessibility:</strong> Available 24/7, no appointment needed, no insurance required</li>
        <li><strong>Perceived comprehensiveness:</strong> AI provides longer, more detailed explanations than time-constrained clinicians often can</li>
        <li><strong>Reduced stigma:</strong> For sensitive topics (mental health, sexual health, embarrassing symptoms), AI feels "safer"</li>
        <li><strong>Cost:</strong> Free or low-cost compared to a physician visit</li>
        <li><strong>Control:</strong> Patients can ask follow-up questions without worrying about taking too much time</li>
      </ul>

      <p>
        Research from NYU found that patients had difficulty distinguishing between AI-generated and
        physician-generated responses to health queries—correctly identifying the source only about 65%
        of the time. This suggests that AI responses are reaching a level of sophistication that makes
        them credible to patients, for better or worse.
      </p>

      <h3>The Limitations Patients Don't See</h3>

      <p>
        While patients perceive AI as comprehensive and knowledgeable, several critical limitations remain
        invisible to non-expert users. First, AI tools like ChatGPT lack real-time medical knowledge—they're
        trained on data with a cutoff date and don't incorporate the latest research, drug alerts, or emerging
        treatment protocols. A patient asking about a recently approved medication may receive outdated or
        simply fabricated information.
      </p>

      <p>
        Second, AI cannot perform a physical examination. The entire diagnostic process in medicine often
        hinges on findings that can only be obtained through direct observation and touch: the texture of
        a skin lesion, the quality of a heart murmur, the tenderness pattern of an abdomen. AI is limited
        to what patients describe—and patients often don't know what's clinically significant to mention.
      </p>

      <p>
        Third, AI lacks the ability to integrate the subtle contextual cues that experienced clinicians
        process automatically: the patient's affect, their living situation, their reliability as a historian,
        the nonverbal signals that suggest whether symptoms are being minimized or amplified. A study by
        symptom checker researchers found that even with perfect information, AI-based tools listed the
        correct diagnosis in the top three options only 51% of the time, compared to 84% for physicians
        given the same vignettes.
      </p>

      <p>
        Finally, AI can hallucinate—generating confident-sounding information that is entirely fabricated.
        Studies have documented AI chatbots inventing medical research, citing papers that don't exist, and
        recommending treatments with no evidence base. Patients have no way to distinguish hallucinated
        information from accurate information without independent verification.
      </p>

      <hr>

      <!-- Part 2 -->
      <h2>Part 2: How AI Health Use Shows Up in Clinical Encounters</h2>

      <h3>The Prepared Patient</h3>

      <p>
        Some patients arrive having done their AI homework in constructive ways. They've generated lists
        of questions, researched their symptoms, and come ready for an informed conversation. An internist
        at Beth Israel Deaconess described a patient who uploaded his lab results to ChatGPT and arrived
        with organized questions. "I welcome patients showing me how they use AI," he noted. "Their research
        creates an opportunity for discussion."
      </p>

      <p>
        These encounters can be efficient and satisfying for both parties. The patient feels heard and
        prepared; the clinician can address specific concerns rather than starting from zero.
      </p>

      <h3>The Worried Well (Now More Worried)</h3>

      <p>
        AI chatbots, when prompted with symptoms, tend to generate comprehensive differential diagnoses—including
        rare and serious conditions. A patient with a headache might receive a list that includes tension
        headache, migraine, sinusitis, but also meningitis, brain tumor, and aneurysm. Without clinical
        context to weight these possibilities, patients may fixate on the most frightening options.
      </p>

      <p>
        An emergency physician who tested ChatGPT on real patient presentations found it performed reasonably
        well for classic presentations with complete information, but noted: "Most actual patient cases are
        not classic. The vast majority of any medical encounter is figuring out the correct patient narrative."
        The AI missed an ectopic pregnancy in a patient presenting with abdominal pain—a potentially fatal
        diagnosis that required the nuanced history-taking that revealed she didn't know she was pregnant.
      </p>

      <h3>The Second Opinion Seeker</h3>

      <p>
        Increasingly, patients use AI to validate—or challenge—their physician's recommendations. This can
        manifest as straightforward questions ("ChatGPT said I should ask about X—what do you think?") or
        more confrontational challenges ("The AI says I don't need this medication—why are you prescribing it?").
      </p>

      <p>
        A Customertimes survey found that 40% of Americans are willing to follow medical advice generated
        by AI. While most still expect AI to serve as a supportive tool rather than a physician replacement,
        the willingness to act on AI recommendations without professional input raises safety concerns.
      </p>

      <h3>The Vulnerable Patient</h3>

      <p>
        Perhaps most concerning: research published in <em>JMIR</em> found that patients failed to distinguish
        potentially harmful AI advice from safe advice. While physicians lowered their ratings for responses
        they identified as harmful, patients' assessments of empathy and usefulness remained unchanged. Most
        harmful responses involved either overtreatment/overdiagnosis or undertreatment/underdiagnosis—errors
        that require clinical expertise to recognize.
      </p>

      <p>
        The study noted that "profound knowledge of the specific field is necessary to identify harmful
        advice"—such as knowing that gallstones greater than 3 cm are associated with increased cancer risk.
        Patients simply don't have the framework to evaluate clinical accuracy.
      </p>

      <h3>Recognizing AI-Influenced Encounters</h3>

      <p>Clinicians should listen for signals that a patient has consulted AI:</p>

      <ul>
        <li>Technical or clinical terminology that seems memorized rather than understood</li>
        <li>References to "reading" about their condition that sound conversational rather than like standard medical websites</li>
        <li>Questions about specific diagnostic possibilities that seem out of proportion to their symptoms</li>
        <li>Requests for tests or treatments "because the AI said..."</li>
        <li>Printed or screenshotted AI conversations</li>
      </ul>

      <p>
        Consider simply asking: "Have you looked this up online or talked to any AI tools about it?"
        Normalizing the question removes stigma and opens dialogue.
      </p>

      <div class="callout callout-tip">
        <div class="callout-title">Case Vignettes</div>
        <p>
          <strong>The Worried Parent:</strong> A mother brings her 4-year-old with a rash. Before you can
          complete your assessment, she shows you a ChatGPT conversation suggesting possible Kawasaki disease,
          Stevens-Johnson syndrome, and scarlet fever. The child has a mild viral exanthem. Your task: validate
          her concern, explain how you assess probability, and demonstrate what clinical findings guide your thinking.
        </p>
        <p>
          <strong>The Medication Skeptic:</strong> A patient with newly diagnosed hypertension returns for
          follow-up, having not filled the lisinopril prescription. He explains that ChatGPT told him about
          potential side effects and suggested "natural alternatives" first. Your task: acknowledge his research,
          explain risk-benefit in context, and have an honest conversation about evidence for alternatives.
        </p>
        <p>
          <strong>The Informed Partner:</strong> A patient with a new cancer diagnosis arrives with her spouse,
          who has spent three days researching treatment options with AI. He's prepared a detailed spreadsheet
          comparing protocols. Some information is accurate; some reflects AI hallucination. Your task: honor
          their preparation, correct misinformation gently, and help them understand how decisions are made.
        </p>
        <p class="mb-0">
          <strong>The Self-Diagnosed Teen:</strong> A 16-year-old has researched her symptoms on multiple AI
          platforms and is convinced she has ADHD. She's compiled supporting evidence and wants medication.
          Your task: take her concerns seriously while conducting appropriate evaluation and exploring alternatives.
        </p>
      </div>

      <hr>

      <!-- Part 3 -->
      <h2>Part 3: Partnering With AI-Using Patients</h2>

      <h3>Reframe the Dynamic</h3>

      <p>
        The instinct to dismiss AI-derived information or feel threatened by it is understandable but
        counterproductive. Patients who research their health—through any medium—are demonstrating engagement.
        That's valuable.
      </p>

      <p>
        Consider the analogy of a patient who arrives having read everything they could find about their
        diagnosis. We generally view this positively, even when their sources are imperfect. AI use is
        similar: it represents patients taking initiative in their healthcare. Our job is to help them
        do it effectively.
      </p>

      <h3>The "Curious Colleague" Approach</h3>

      <p>
        Rather than positioning yourself as the authority correcting AI mistakes, try approaching the
        conversation as a collaborative evaluation of information. Some phrases that work:
      </p>

      <ul>
        <li>"That's interesting—let's look at what the AI told you and see how it applies to your specific situation."</li>
        <li>"AI can be helpful for general information. What it can't do is examine you or know your full history. Let me show you what I'm seeing that changes the picture."</li>
        <li>"You've done some good research. Let me help you figure out which parts apply to you."</li>
        <li>"The AI gave you a good starting list. Here's how I'm thinking about narrowing it down based on your exam."</li>
      </ul>

      <h3>Addressing Discrepancies</h3>

      <p>When your recommendation differs from what the AI suggested, transparency is key:</p>

      <ol>
        <li><strong>Acknowledge the AI's reasoning:</strong> "Based on symptoms alone, the AI's suggestion makes sense."</li>
        <li><strong>Explain your additional data:</strong> "But I can see that your exam shows X, your history includes Y, and that changes the probability significantly."</li>
        <li><strong>Make your reasoning visible:</strong> "Here's why I'm less worried about the scary diagnosis and more focused on..."</li>
        <li><strong>Invite questions:</strong> "Does that help explain why I'm thinking differently? What other questions do you have?"</li>
      </ol>

      <h3>When the AI Was Right (Or Partially Right)</h3>

      <p>
        Sometimes patients bring AI-generated insights that are genuinely useful—a medication interaction
        you might have missed, a question about a relevant clinical trial, or a reasonable concern that
        warrants investigation. Acknowledge this:
      </p>

      <ul>
        <li>"Good catch—that's worth checking."</li>
        <li>"The AI actually raised a reasonable point. Let me look into that."</li>
        <li>"I hadn't considered that angle—I'm glad you brought it up."</li>
      </ul>

      <p>
        This builds trust and models the appropriate use of AI as a supplement to—not replacement for—clinical judgment.
      </p>

      <h3>Communication Scripts for Common Situations</h3>

      <div class="callout callout-info">
        <div class="callout-title">When AI information is accurate but incomplete</div>
        <p class="mb-0">
          "The AI gave you accurate general information about [condition]. What it couldn't know is your
          specific situation—your other health conditions, your medications, your lifestyle. Let me fill
          in how this applies to you specifically, because the treatment approach really depends on those details."
        </p>
      </div>

      <div class="callout callout-info">
        <div class="callout-title">When AI information is outdated</div>
        <p class="mb-0">
          "That recommendation was probably accurate a few years ago. The guidelines have actually changed—we
          now know that [updated approach] is more effective/safer. AI tools don't always have the most current
          information, which is one reason it's important to verify things with your healthcare team."
        </p>
      </div>

      <div class="callout callout-warning">
        <div class="callout-title">When AI information is simply wrong</div>
        <p class="mb-0">
          "I can see why that seemed convincing, but I need to correct something important. AI tools sometimes
          generate information that sounds authoritative but isn't accurate—it's called 'hallucination.' In
          this case, [correct information]. This is exactly why these tools work best as a starting point for
          questions rather than as a final answer."
        </p>
      </div>

      <div class="callout callout-warning">
        <div class="callout-title">When a patient wants to follow AI over your recommendation</div>
        <p class="mb-0">
          "I respect that you've done a lot of research, and I want to make sure you understand my reasoning
          so you can make an informed choice. Here's why I'm recommending [approach] instead of what the AI
          suggested: [specific reasons]. Ultimately, this is your health and your decision. I want to make
          sure you have complete information, including what concerns me about the alternative approach and
          what we would watch for if you choose to go that route."
        </p>
      </div>

      <h3>Managing Anxiety Amplified by AI</h3>

      <p>For patients whose AI research has increased their anxiety, the clinical approach matters:</p>

      <ol>
        <li><strong>Validate the concern:</strong> "It's understandable that seeing 'brain tumor' on a list would be frightening."</li>
        <li><strong>Explain probability:</strong> "AI lists everything possible, but doesn't tell you what's likely. For your symptoms, in your age group, with your exam, here's what's much more probable..."</li>
        <li><strong>Provide concrete reassurance:</strong> "Here's specifically what I looked for on your exam that tells me we don't need to worry about X."</li>
        <li><strong>Create a safety net:</strong> "If you develop any of these specific symptoms, that would change my thinking and you should call right away."</li>
      </ol>

      <hr>

      <!-- Part 4 -->
      <h2>Part 4: Teaching Patients to Use AI More Effectively</h2>

      <p>
        Rather than discouraging AI use—which is unlikely to work and may damage trust—consider equipping
        patients with skills to use these tools more safely. This represents a form of digital health
        literacy that's becoming as important as understanding how to read a nutrition label.
      </p>

      <h3>Core Principles for Patients</h3>

      <h4>1. AI Is a Starting Point, Not a Destination</h4>

      <p>
        Help patients understand that AI can be useful for generating questions, understanding terminology,
        or exploring possibilities—but it cannot examine them, know their full history, or make clinical
        decisions. The appropriate endpoint for serious health concerns is always a qualified professional.
      </p>

      <h4>2. Prompting Matters</h4>

      <p>
        Research shows that the quality of AI responses depends heavily on how questions are framed.
        Teaching patients basic prompting skills can improve the information they receive:
      </p>

      <ul>
        <li>Include relevant personal context (age, relevant medical history, current medications)</li>
        <li>Ask the AI to "act as a clinician" or "explain as if to a patient"</li>
        <li>Ask one focused question at a time rather than multiple questions</li>
        <li>Request that the AI explain its reasoning</li>
        <li>Ask for counterarguments or alternative explanations</li>
      </ul>

      <h4>3. Privacy Isn't Guaranteed</h4>

      <p>
        Consumer AI tools are not HIPAA-compliant. Data entered into ChatGPT, Claude, or similar tools
        goes to companies that may use it for training or other purposes. Patients should avoid including
        identifying information (name, date of birth, Social Security number) in health queries.
      </p>

      <h4>4. AI Can Hallucinate</h4>

      <p>
        Large language models can generate confident-sounding but entirely fabricated information. Patients
        should be especially skeptical of specific claims (drug interactions, dosages, treatment protocols)
        and verify anything important with a professional or authoritative medical source.
      </p>

      <h4>5. Emergency Symptoms Require Emergency Care</h4>

      <p>
        AI is not appropriate for evaluating urgent symptoms. Patients should understand that chest pain,
        severe shortness of breath, neurological changes, or other potentially serious symptoms require
        immediate professional evaluation—not an AI conversation.
      </p>

      <h3>Practical Tips for Patient AI Use</h3>

      <p>Consider providing patients with these concrete strategies:</p>

      <div class="callout callout-tip">
        <div class="callout-title">Better Ways to Use AI for Health Information</div>
        <ul class="mb-0">
          <li><strong>Understanding diagnoses:</strong> "Explain [condition] in simple terms—what causes it, how it's treated, and what to expect."</li>
          <li><strong>Preparing for appointments:</strong> "What questions should I ask my doctor about [condition/medication/test]?"</li>
          <li><strong>Understanding lab results:</strong> "What does a [specific test] measure and what do high/low values generally indicate?"</li>
          <li><strong>Medication information:</strong> "What are common side effects of [medication] and when should I be concerned?"</li>
          <li><strong>Learning terminology:</strong> "What does [medical term] mean in plain language?"</li>
        </ul>
      </div>

      <div class="callout callout-warning">
        <div class="callout-title">Riskier Ways to Use AI (Proceed With Caution)</div>
        <ul class="mb-0">
          <li>Self-diagnosing based on symptom descriptions</li>
          <li>Making treatment decisions without professional input</li>
          <li>Relying on AI for medication dosing or interactions</li>
          <li>Using AI during potential emergencies</li>
          <li>Trusting AI recommendations over clinical advice</li>
        </ul>
      </div>

      <h3>The "Verify Big Claims" Rule</h3>

      <p>
        As one physician advises: "If it tells you something that you think is really big, verify it with
        an expert human." This simple heuristic captures much of what patients need to know. AI can be useful
        for routine questions and background information. For anything significant—a worrisome symptom,
        a treatment decision, a major diagnosis—professional verification is essential.
      </p>

      <hr>

      <!-- Part 5 -->
      <h2>Part 5: Special Populations and Considerations</h2>

      <h3>Equity Considerations</h3>

      <p>
        The intersection of AI health tools and healthcare disparities deserves attention. AI use both
        reflects and potentially exacerbates existing inequities.
      </p>

      <p>
        The finding that AI health tool users have lower educational attainment and rely more on emergency/urgent
        care settings suggests these tools may serve as a workaround for patients with less access to traditional
        primary care. According to the American Hospital Association, over 26% of Medicare beneficiaries lack
        home computer or smartphone access, and more than 22% of rural Americans lack adequate broadband.
      </p>

      <p>
        AI systems are trained on data that reflects historical patterns—including healthcare disparities.
        Populations that have been historically marginalized are underrepresented in training datasets,
        potentially leading to less accurate or relevant information for these patients.
      </p>

      <h3>Pediatric and Adolescent Considerations</h3>

      <p>Parents researching their children's health present unique dynamics:</p>

      <ul>
        <li><strong>Developmental information:</strong> Parents frequently query AI about milestones, autism spectrum features, ADHD symptoms. AI may provide generic checklists without context about normal variation.</li>
        <li><strong>Adolescent self-diagnosis:</strong> Teenagers increasingly use AI to research mental health conditions and physical symptoms. This can represent healthy self-awareness or lead to premature self-labeling.</li>
        <li><strong>Privacy for adolescents:</strong> Teens may use AI for health questions they're uncomfortable asking parents or physicians. Consider asking adolescents directly about AI health use in confidential portions of visits.</li>
      </ul>

      <h3>Mental Health Specific Concerns</h3>

      <p>AI use for mental health information deserves particular attention:</p>

      <ul>
        <li>AI tools may underestimate suicide risk, with some studies showing AI failing to appropriately escalate urgent safety concerns</li>
        <li>Self-diagnosis of personality disorders or complex trauma can be harmful without professional assessment</li>
        <li>AI-generated coping strategies may be generic and miss patient-specific contraindications</li>
        <li>Patients in crisis should not be relying on AI chatbots—ensure patients know crisis resources</li>
      </ul>

      <h3>Chronic Disease Management</h3>

      <p>
        Patients with chronic conditions present particular opportunities and risks. These patients have
        ongoing need for information and may make daily self-management decisions.
      </p>

      <p><strong>Guidance for chronic disease patients using AI:</strong></p>

      <ul>
        <li>Use AI for understanding and tracking, not for treatment changes</li>
        <li>Bring AI-generated questions to your clinical team</li>
        <li>Never adjust insulin, anticoagulation, or other high-risk medications based on AI advice</li>
        <li>Use AI to help communicate symptoms accurately to your healthcare team</li>
      </ul>

      <hr>

      <!-- Part 6 -->
      <h2>Part 6: The Opportunity</h2>

      <p>Patient AI use isn't going away. The question for clinicians is whether to view it as an obstacle or an opening.</p>

      <p>
        <em>Patients who consult AI are engaged in their health.</em> They're taking initiative, seeking
        information, and thinking about their conditions between visits. This is exactly what we want from
        patients—active participation in their healthcare.
      </p>

      <p>
        <em>AI can surface questions patients wouldn't otherwise ask.</em> When a patient brings an AI-generated
        list of considerations, they may raise issues that wouldn't have come up in a routine visit.
      </p>

      <p>
        <em>Teaching AI literacy is a form of patient education.</em> Helping patients use AI tools effectively
        reinforces critical thinking about health information generally.
      </p>

      <p>
        <em>Transparency about AI builds trust.</em> When clinicians openly discuss AI's strengths and
        limitations—rather than dismissing it—they demonstrate honesty and respect for patients' autonomy.
      </p>

      <h3>Building AI Literacy Into Your Practice</h3>

      <p>Consider how you might systematically address AI use:</p>

      <ul>
        <li><strong>Intake forms:</strong> Add a question about what resources patients have consulted, including AI tools</li>
        <li><strong>Waiting room materials:</strong> Provide guidance on using AI tools effectively and their limitations</li>
        <li><strong>Visit routine:</strong> Normalize asking about AI use as part of history-taking</li>
        <li><strong>After-visit materials:</strong> Recommend trusted resources as alternatives to unvetted AI queries</li>
        <li><strong>Staff training:</strong> Ensure all team members can respond to patient questions consistently</li>
      </ul>

      <h3>Documentation Considerations</h3>

      <p>When patient AI use is relevant to clinical decision-making, consider documenting it:</p>

      <ul>
        <li><strong>Note AI-influenced concerns:</strong> "Patient expressed concern about [condition] based on AI chatbot research. Discussed differential diagnosis and clinical reasoning."</li>
        <li><strong>Document education provided:</strong> "Discussed appropriate use of AI health tools, including limitations."</li>
        <li><strong>Record disagreements:</strong> "Patient preferred alternative approach based on AI recommendation. Discussed risks and benefits."</li>
      </ul>

      <hr>

      <!-- Quick Reference -->
      <h2>Quick Reference: When Patients Bring AI to the Visit</h2>

      <table class="comparison-table">
        <thead>
          <tr>
            <th>Scenario</th>
            <th>Approach</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Patient is prepared and informed</strong></td>
            <td>Acknowledge their preparation; use as foundation for discussion; identify areas needing clarification</td>
          </tr>
          <tr>
            <td><strong>Patient is anxious about AI findings</strong></td>
            <td>Validate concern; explain probability vs. possibility; show exam findings that provide reassurance; create safety net</td>
          </tr>
          <tr>
            <td><strong>AI recommendation differs from yours</strong></td>
            <td>Acknowledge AI logic; explain your additional data (exam, history); make reasoning transparent; invite questions</td>
          </tr>
          <tr>
            <td><strong>AI raised a valid point</strong></td>
            <td>Acknowledge openly; investigate if warranted; model appropriate use of AI as supplement to clinical judgment</td>
          </tr>
          <tr>
            <td><strong>Patient challenging your recommendation</strong></td>
            <td>Stay curious not defensive; explore what the AI said; explain clinical reasoning; respect patient autonomy while ensuring informed decision</td>
          </tr>
        </tbody>
      </table>

      <h3>Key Messages for Patients</h3>

      <ol>
        <li>AI is a starting point, not a diagnosis.</li>
        <li>Better prompts yield better information.</li>
        <li>Your data isn't private—avoid identifying information.</li>
        <li>AI can hallucinate—verify important claims.</li>
        <li>Emergency symptoms need emergency care, not AI.</li>
      </ol>

      <div class="callout callout-tip">
        <div class="callout-title">The Bottom Line</div>
        <p class="mb-0">
          Patients who consult AI are engaged in their health. Meet them where they are, help them use
          these tools wisely, and leverage their research as an opportunity for deeper conversation and better care.
        </p>
      </div>

      <hr>

      <!-- Resources -->
      <h2>Resources</h2>

      <h3>Key Research</h3>

      <ul>
        <li>Ayo-Ajibola O, et al. "Characterizing the Adoption and Experiences of Users of Artificial Intelligence–Generated Health Information in the United States." <em>J Med Internet Res</em> 2024;26:e55138</li>
        <li>"Doctor ChatGPT, Can You Help Me? The Patient's Perspective: Cross-Sectional Study." <em>J Med Internet Res</em> 2024</li>
        <li>"Utilisation of AI-driven chatbots for perioperative health information seeking." <em>BMJ Open</em> 2025</li>
      </ul>

      <h3>Guidelines and Frameworks</h3>

      <ul>
        <li>World Health Organization: Ethics and Governance of Artificial Intelligence for Health</li>
        <li>FDA: Artificial Intelligence and Machine Learning in Software as a Medical Device</li>
        <li>NCBI Bookshelf: Chatbots in Health Care: Connecting Patients to Information</li>
      </ul>

      <h3>Patient-Facing Resources</h3>

      <ul>
        <li>MedlinePlus: Trusted patient health information for comparison with AI responses</li>
        <li>OpenNotes: Research on patient access to clinical notes and AI interpretation</li>
      </ul>

      <!-- Learning Objectives -->
      <div class="learning-objectives">
        <h3>Learning Objectives</h3>
        <p>After completing this topic, you should be able to:</p>
        <ul>
          <li>Recognize patterns of patient AI use in clinical encounters</li>
          <li>Apply communication strategies for partnering with AI-using patients</li>
          <li>Address discrepancies between AI-generated information and clinical recommendations</li>
          <li>Teach patients to use AI health tools more effectively and safely</li>
          <li>Identify special considerations for vulnerable populations</li>
          <li>Transform patient AI use into opportunities for engagement and education</li>
        </ul>
      </div>

      <!-- Page Navigation -->
      <nav class="page-nav">
        <a href="everyday-ai.html" class="page-nav-link prev">
          <span class="page-nav-arrow"><i data-lucide="arrow-left"></i></span>
          <div>
            <span class="page-nav-label">Previous</span>
            <span class="page-nav-title">Everyday Ways to Use AI</span>
          </div>
        </a>
        <a href="vibe-coding.html" class="page-nav-link next">
          <span class="page-nav-arrow"><i data-lucide="arrow-right"></i></span>
          <div>
            <span class="page-nav-label">Next</span>
            <span class="page-nav-title">Vibe Coding</span>
          </div>
        </a>
      </nav>

    </article>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      <div><strong>AI 101</strong> · A Self-Paced Guide to AI in Medicine</div>
      <div>v1.0 · 2025</div>
    </div>
  </footer>

  <script>
    lucide.createIcons();
    const navToggle = document.querySelector('.nav-toggle');
    const navLinks = document.querySelector('.nav-links');
    if (navToggle) {
      navToggle.addEventListener('click', () => {
        navLinks.classList.toggle('nav-open');
        navToggle.classList.toggle('nav-toggle-active');
      });
    }
  </script>

</body>
</html>
