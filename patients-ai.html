<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>When Patients Use AI Too | AI 101</title>
  <link rel="stylesheet" href="styles.css">
  <script src="https://unpkg.com/lucide@latest"></script>
</head>
<body>

  <nav class="nav">
    <div class="nav-inner">
      <a href="index.html" class="nav-brand">
        <span class="nav-badge">AI 101</span>
        <span class="nav-title">A Self-Paced Guide to AI in Medicine</span>
      </a>
      <button class="nav-toggle" aria-label="Toggle menu">
        <i data-lucide="menu"></i>
      </button>
      <div class="nav-links">
        <a href="index.html" class="nav-link">Topics</a>
        <a href="about.html" class="nav-link">About</a>
        <a href="contribute.html" class="nav-link">Contribute</a>
        <div id="deployment-eac8a421-3c54-4717-bbef-54bb81b54243" class="nav-chatbot"></div>
      </div>
    </div>
  </nav>

  <main class="main">
    <article class="content content-wide">

      <header class="unit-header">
        <span class="unit-label phase-2">USING AI</span>
        <h1 class="unit-title">When Patients Bring AI to the Exam Room</h1>
        <p class="unit-subtitle">
          Understanding, partnering with, and guiding patient use of AI health tools.
        </p>
        <div class="unit-meta">
          <span class="unit-meta-item">
            <i data-lucide="clock"></i>
            ~35 min read
          </span>
          <span class="unit-meta-item">
            <i data-lucide="users"></i>
            Patient communication
          </span>
        </div>
      </header>

      <div class="callout callout-question">
        <div class="callout-title">Core Question</div>
        <p class="mb-0">
          How can you transform patient AI use from a clinical challenge into an opportunity
          for deeper engagement and better outcomes?
        </p>
      </div>

      <!-- Prioritize This -->
      <div class="callout callout-tip" style="border-left-color: #16a34a; background: #f0fdf4;">
        <div class="callout-title" style="color: #16a34a;">Prioritize This!</div>
        <p>
          If you take one thing from this topic: <strong>start asking about AI use without
          judgment</strong>. Add this question to your routine: <em>"Have you looked this up
          online or talked to any AI tools about it? I'd love to hear what you found."</em>
        </p>
        <p class="mb-0">
          This simple question—asked with genuine curiosity, not suspicion—opens dialogue,
          builds trust, and gives you insight into what your patient is thinking. Patients
          who feel judged for using AI will hide it. Patients who feel heard will share it.
        </p>
      </div>

      <!-- Introduction -->
      <h2>Introduction: The New Reality</h2>

      <p>
        Your patient walks in with a printout from ChatGPT suggesting she might have lupus. Another asks
        why you didn't order the MRI that "the AI" recommended. A parent wants to know if the treatment
        plan you've outlined matches what Google's Gemini told them about their child's ear infection.
        These scenarios are no longer hypothetical—they're becoming routine.
      </p>

      <p>
        According to a 2024 KFF poll, approximately 17% of U.S. adults now use AI chatbots at least monthly
        for health information and advice, with that figure climbing to 25% among adults under 30. A
        cross-sectional study published in the <em>Journal of Medical Internet Research</em> found that
        21.5% of respondents had used ChatGPT specifically for online health information. The Elsevier
        Clinician of the Future 2025 survey reveals a striking mismatch: while patients increasingly turn
        to AI tools, only 16% of clinicians report using AI in direct decision-making, and just 30% feel
        they've received adequate training.
      </p>

      <p>
        This topic examines the emerging landscape of patient AI use, how it manifests in clinical
        encounters, and—most importantly—how clinicians can transform what might feel like a challenge
        into an opportunity for deeper engagement and better outcomes.
      </p>

      <hr>

      <!-- Part 1 -->
      <h2>Part 1: Understanding How Patients Use AI</h2>

      <h3>Who Is Using AI for Health Information?</h3>

      <p>
        The demographic profile of health AI users challenges some assumptions. Research from ResearchMatch
        reveals that ChatGPT users for health information tend to be younger (average age 32.8 versus 39.1
        for non-users), but—perhaps surprisingly—have <em>lower</em> rates of advanced degree attainment
        (49.9% versus 67% holding a bachelor's degree or higher). These users also show greater utilization
        of transient healthcare settings like emergency departments and urgent care.
      </p>

      <p>
        This pattern suggests that AI health tools may be filling gaps where traditional primary care access
        falls short. Patients who lack a consistent primary care relationship, face longer wait times, or
        experience cost barriers may turn to AI as a more accessible—if imperfect—alternative.
      </p>

      <h3>What Are Patients Asking AI?</h3>

      <p>Patient AI queries cluster into several distinct categories:</p>

      <ul>
        <li><strong>Symptom interpretation and self-diagnosis:</strong> "I have a headache behind my eyes and a stiff neck—what could this be?"</li>
        <li><strong>Lab result explanation:</strong> Uploading MyChart screenshots and asking what their numbers mean</li>
        <li><strong>Medication questions:</strong> Side effects, interactions, whether a prescribed medication is "really necessary"</li>
        <li><strong>Treatment validation:</strong> Checking whether their doctor's recommendation aligns with what the AI suggests</li>
        <li><strong>Understanding diagnoses:</strong> "What is rheumatoid arthritis and what should I expect?"</li>
        <li><strong>Preparing for appointments:</strong> Generating questions to ask, understanding what tests might be ordered</li>
      </ul>

      <p>
        A study examining AI chatbot use for perioperative health information found four key themes: patients
        prefer AI chatbots over traditional search engines for their conversational interface, they appreciate
        the improved accessibility and perceived quality of information, they sometimes prefer AI interaction
        over human interaction for sensitive questions, and they recognize that effective prompting skills
        matter for getting useful responses.
      </p>

      <h3>Why Patients Choose AI Over Traditional Sources</h3>

      <p>
        The appeal is understandable when you consider the patient's perspective. As one patient told the
        <em>New York Times</em>: "ChatGPT has all day for me—it never rushes me out of the chat." This
        reflects a fundamental tension in modern healthcare: the average primary care visit lasts about
        18 minutes, with much of that time consumed by documentation and administrative tasks. AI offers
        unlimited time, immediate availability, and—crucially—no judgment.
      </p>

      <p>Research on patient motivations identifies several drivers:</p>

      <ul>
        <li><strong>Accessibility:</strong> Available 24/7, no appointment needed, no insurance required</li>
        <li><strong>Perceived comprehensiveness:</strong> AI provides longer, more detailed explanations than time-constrained clinicians often can</li>
        <li><strong>Reduced stigma:</strong> For sensitive topics (mental health, sexual health, embarrassing symptoms), AI feels "safer"</li>
        <li><strong>Cost:</strong> Free or low-cost compared to a physician visit</li>
        <li><strong>Control:</strong> Patients can ask follow-up questions without worrying about taking too much time</li>
      </ul>

      <p>
        Research from NYU found that patients had difficulty distinguishing between AI-generated and
        physician-generated responses to health queries—correctly identifying the source only about 65%
        of the time. This suggests that AI responses are reaching a level of sophistication that makes
        them credible to patients, for better or worse.
      </p>

      <h3>Why Patients Trust AI—And What It Teaches Us</h3>

      <p>
        Here's an uncomfortable truth that the research reveals: <strong>AI chatbots often outperform
        physicians in perceived empathy</strong>. A landmark
        <a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2804309" target="_blank">JAMA Internal Medicine study</a>
        found that healthcare professionals preferred ChatGPT responses over physician responses 79% of
        the time. Even more striking: empathetic responses were <em>9.8 times more common</em> from ChatGPT
        (45.1%) than from physicians (4.6%).
      </p>

      <p>Why? The differences are instructive:</p>

      <ul>
        <li><strong>Length:</strong> ChatGPT responses averaged 168-245 words; physician responses averaged 17-62 words</li>
        <li><strong>Validation:</strong> AI consistently acknowledges patient concerns before providing information</li>
        <li><strong>Patience:</strong> AI never rushes, never seems annoyed by "obvious" questions, never makes patients feel burdensome</li>
        <li><strong>Availability:</strong> AI is there at 2 AM when anxiety peaks, with no wait time or cost</li>
      </ul>

      <div class="callout callout-principle">
        <div class="callout-title">The Opportunity for Providers</div>
        <p>
          This isn't about AI replacing physicians—it's about AI revealing what patients crave:
          <strong>time, validation, and explanation</strong>. When a patient turns to ChatGPT, they're
          often seeking what a rushed 18-minute visit couldn't provide.
        </p>
        <p>
          The research suggests a path forward: physicians using AI to draft responses produced messages
          that were 2-3 times longer and rated more empathetic, while reporting decreased cognitive burden.
          AI can help us be more human, not less.
        </p>
        <p class="mb-0">
          <strong>Ask yourself:</strong> When a patient brings AI research to your visit, are they
          challenging your expertise—or showing you what they needed that they didn't get elsewhere?
        </p>
      </div>

      <h3>The Limitations Patients Don't See</h3>

      <p>
        While patients perceive AI as comprehensive and knowledgeable, several critical limitations remain
        invisible to non-expert users. First, AI tools like ChatGPT lack real-time medical knowledge—they're
        trained on data with a cutoff date and don't incorporate the latest research, drug alerts, or emerging
        treatment protocols. A patient asking about a recently approved medication may receive outdated or
        simply fabricated information.
      </p>

      <p>
        Second, AI cannot perform a physical examination. The entire diagnostic process in medicine often
        hinges on findings that can only be obtained through direct observation and touch: the texture of
        a skin lesion, the quality of a heart murmur, the tenderness pattern of an abdomen. AI is limited
        to what patients describe—and patients often don't know what's clinically significant to mention.
      </p>

      <p>
        Third, AI lacks the ability to integrate the subtle contextual cues that experienced clinicians
        process automatically: the patient's affect, their living situation, their reliability as a historian,
        the nonverbal signals that suggest whether symptoms are being minimized or amplified. A study by
        symptom checker researchers found that even with perfect information, AI-based tools listed the
        correct diagnosis in the top three options only 51% of the time, compared to 84% for physicians
        given the same vignettes.
      </p>

      <p>
        Finally, AI can hallucinate—generating confident-sounding information that is entirely fabricated.
        Studies have documented AI chatbots inventing medical research, citing papers that don't exist, and
        recommending treatments with no evidence base. Patients have no way to distinguish hallucinated
        information from accurate information without independent verification.
      </p>

      <hr>

      <!-- Part 2 -->
      <h2>Part 2: How AI Health Use Shows Up in Clinical Encounters</h2>

      <h3>The Prepared Patient</h3>

      <p>
        Some patients arrive having done their AI homework in constructive ways. They've generated lists
        of questions, researched their symptoms, and come ready for an informed conversation. An internist
        at Beth Israel Deaconess described a patient who uploaded his lab results to ChatGPT and arrived
        with organized questions. "I welcome patients showing me how they use AI," he noted. "Their research
        creates an opportunity for discussion."
      </p>

      <p>
        These encounters can be efficient and satisfying for both parties. The patient feels heard and
        prepared; the clinician can address specific concerns rather than starting from zero.
      </p>

      <h3>The Worried Well (Now More Worried)</h3>

      <p>
        AI chatbots, when prompted with symptoms, tend to generate comprehensive differential diagnoses—including
        rare and serious conditions. A patient with a headache might receive a list that includes tension
        headache, migraine, sinusitis, but also meningitis, brain tumor, and aneurysm. Without clinical
        context to weight these possibilities, patients may fixate on the most frightening options.
      </p>

      <p>
        An emergency physician who tested ChatGPT on real patient presentations found it performed reasonably
        well for classic presentations with complete information, but noted: "Most actual patient cases are
        not classic. The vast majority of any medical encounter is figuring out the correct patient narrative."
        The AI missed an ectopic pregnancy in a patient presenting with abdominal pain—a potentially fatal
        diagnosis that required the nuanced history-taking that revealed she didn't know she was pregnant.
      </p>

      <h3>The Second Opinion Seeker</h3>

      <p>
        Increasingly, patients use AI to validate—or challenge—their physician's recommendations. This can
        manifest as straightforward questions ("ChatGPT said I should ask about X—what do you think?") or
        more confrontational challenges ("The AI says I don't need this medication—why are you prescribing it?").
      </p>

      <p>
        A Customertimes survey found that 40% of Americans are willing to follow medical advice generated
        by AI. While most still expect AI to serve as a supportive tool rather than a physician replacement,
        the willingness to act on AI recommendations without professional input raises safety concerns.
      </p>

      <h3>The Vulnerable Patient</h3>

      <p>
        Perhaps most concerning: research published in <em>JMIR</em> found that patients failed to distinguish
        potentially harmful AI advice from safe advice. While physicians lowered their ratings for responses
        they identified as harmful, patients' assessments of empathy and usefulness remained unchanged. Most
        harmful responses involved either overtreatment/overdiagnosis or undertreatment/underdiagnosis—errors
        that require clinical expertise to recognize.
      </p>

      <p>
        The study noted that "profound knowledge of the specific field is necessary to identify harmful
        advice"—such as knowing that gallstones greater than 3 cm are associated with increased cancer risk.
        Patients simply don't have the framework to evaluate clinical accuracy.
      </p>

      <h3>Recognizing AI-Influenced Encounters</h3>

      <p>Clinicians should listen for signals that a patient has consulted AI:</p>

      <ul>
        <li>Technical or clinical terminology that seems memorized rather than understood</li>
        <li>References to "reading" about their condition that sound conversational rather than like standard medical websites</li>
        <li>Questions about specific diagnostic possibilities that seem out of proportion to their symptoms</li>
        <li>Requests for tests or treatments "because the AI said..."</li>
        <li>Printed or screenshotted AI conversations</li>
      </ul>

      <p>
        Consider simply asking: "Have you looked this up online or talked to any AI tools about it?"
        Normalizing the question removes stigma and opens dialogue.
      </p>

      <div class="callout callout-tip">
        <div class="callout-title">Case Vignettes</div>
        <p>
          <strong>The Worried Parent:</strong> A mother brings her 4-year-old with a rash. Before you can
          complete your assessment, she shows you a ChatGPT conversation suggesting possible Kawasaki disease,
          Stevens-Johnson syndrome, and scarlet fever. The child has a mild viral exanthem. Your task: validate
          her concern, explain how you assess probability, and demonstrate what clinical findings guide your thinking.
        </p>
        <p>
          <strong>The Medication Skeptic:</strong> A patient with newly diagnosed hypertension returns for
          follow-up, having not filled the lisinopril prescription. He explains that ChatGPT told him about
          potential side effects and suggested "natural alternatives" first. Your task: acknowledge his research,
          explain risk-benefit in context, and have an honest conversation about evidence for alternatives.
        </p>
        <p>
          <strong>The Informed Partner:</strong> A patient with a new cancer diagnosis arrives with her spouse,
          who has spent three days researching treatment options with AI. He's prepared a detailed spreadsheet
          comparing protocols. Some information is accurate; some reflects AI hallucination. Your task: honor
          their preparation, correct misinformation gently, and help them understand how decisions are made.
        </p>
        <p class="mb-0">
          <strong>The Self-Diagnosed Teen:</strong> A 16-year-old has researched her symptoms on multiple AI
          platforms and is convinced she has ADHD. She's compiled supporting evidence and wants medication.
          Your task: take her concerns seriously while conducting appropriate evaluation and exploring alternatives.
        </p>
      </div>

      <hr>

      <!-- Part 3 -->
      <h2>Part 3: Partnering With AI-Using Patients</h2>

      <h3>Reframe the Dynamic</h3>

      <p>
        The instinct to dismiss AI-derived information or feel threatened by it is understandable but
        counterproductive. Patients who research their health—through any medium—are demonstrating engagement.
        That's valuable.
      </p>

      <p>
        Consider the analogy of a patient who arrives having read everything they could find about their
        diagnosis. We generally view this positively, even when their sources are imperfect. AI use is
        similar: it represents patients taking initiative in their healthcare. Our job is to help them
        do it effectively.
      </p>

      <h3>The "Curious Colleague" Approach</h3>

      <p>
        Rather than positioning yourself as the authority correcting AI mistakes, try approaching the
        conversation as a collaborative evaluation of information. Some phrases that work:
      </p>

      <ul>
        <li>"That's interesting—let's look at what the AI told you and see how it applies to your specific situation."</li>
        <li>"AI can be helpful for general information. What it can't do is examine you or know your full history. Let me show you what I'm seeing that changes the picture."</li>
        <li>"You've done some good research. Let me help you figure out which parts apply to you."</li>
        <li>"The AI gave you a good starting list. Here's how I'm thinking about narrowing it down based on your exam."</li>
      </ul>

      <h3>Addressing Discrepancies</h3>

      <p>When your recommendation differs from what the AI suggested, transparency is key:</p>

      <ol>
        <li><strong>Acknowledge the AI's reasoning:</strong> "Based on symptoms alone, the AI's suggestion makes sense."</li>
        <li><strong>Explain your additional data:</strong> "But I can see that your exam shows X, your history includes Y, and that changes the probability significantly."</li>
        <li><strong>Make your reasoning visible:</strong> "Here's why I'm less worried about the scary diagnosis and more focused on..."</li>
        <li><strong>Invite questions:</strong> "Does that help explain why I'm thinking differently? What other questions do you have?"</li>
      </ol>

      <h3>When the AI Was Right (Or Partially Right)</h3>

      <p>
        Sometimes patients bring AI-generated insights that are genuinely useful—a medication interaction
        you might have missed, a question about a relevant clinical trial, or a reasonable concern that
        warrants investigation. Acknowledge this:
      </p>

      <ul>
        <li>"Good catch—that's worth checking."</li>
        <li>"The AI actually raised a reasonable point. Let me look into that."</li>
        <li>"I hadn't considered that angle—I'm glad you brought it up."</li>
      </ul>

      <p>
        This builds trust and models the appropriate use of AI as a supplement to—not replacement for—clinical judgment.
      </p>

      <h3>Communication Scripts for Common Situations</h3>

      <div class="callout callout-info">
        <div class="callout-title">When AI information is accurate but incomplete</div>
        <p class="mb-0">
          "The AI gave you accurate general information about [condition]. What it couldn't know is your
          specific situation—your other health conditions, your medications, your lifestyle. Let me fill
          in how this applies to you specifically, because the treatment approach really depends on those details."
        </p>
      </div>

      <div class="callout callout-info">
        <div class="callout-title">When AI information is outdated</div>
        <p class="mb-0">
          "That recommendation was probably accurate a few years ago. The guidelines have actually changed—we
          now know that [updated approach] is more effective/safer. AI tools don't always have the most current
          information, which is one reason it's important to verify things with your healthcare team."
        </p>
      </div>

      <div class="callout callout-warning">
        <div class="callout-title">When AI information is simply wrong</div>
        <p class="mb-0">
          "I can see why that seemed convincing, but I need to correct something important. AI tools sometimes
          generate information that sounds authoritative but isn't accurate—it's called 'hallucination.' In
          this case, [correct information]. This is exactly why these tools work best as a starting point for
          questions rather than as a final answer."
        </p>
      </div>

      <div class="callout callout-warning">
        <div class="callout-title">When a patient wants to follow AI over your recommendation</div>
        <p class="mb-0">
          "I respect that you've done a lot of research, and I want to make sure you understand my reasoning
          so you can make an informed choice. Here's why I'm recommending [approach] instead of what the AI
          suggested: [specific reasons]. Ultimately, this is your health and your decision. I want to make
          sure you have complete information, including what concerns me about the alternative approach and
          what we would watch for if you choose to go that route."
        </p>
      </div>

      <h3>Managing Anxiety Amplified by AI</h3>

      <p>For patients whose AI research has increased their anxiety, the clinical approach matters:</p>

      <ol>
        <li><strong>Validate the concern:</strong> "It's understandable that seeing 'brain tumor' on a list would be frightening."</li>
        <li><strong>Explain probability:</strong> "AI lists everything possible, but doesn't tell you what's likely. For your symptoms, in your age group, with your exam, here's what's much more probable..."</li>
        <li><strong>Provide concrete reassurance:</strong> "Here's specifically what I looked for on your exam that tells me we don't need to worry about X."</li>
        <li><strong>Create a safety net:</strong> "If you develop any of these specific symptoms, that would change my thinking and you should call right away."</li>
      </ol>

      <hr>

      <!-- Part 4 -->
      <h2>Part 4: Teaching Patients to Use AI More Effectively</h2>

      <p>
        Rather than discouraging AI use—which is unlikely to work and may damage trust—consider equipping
        patients with skills to use these tools more safely. This represents a form of digital health
        literacy that's becoming as important as understanding how to read a nutrition label.
      </p>

      <h3>Core Principles for Patients</h3>

      <h4>1. AI Is a Starting Point, Not a Destination</h4>

      <p>
        Help patients understand that AI can be useful for generating questions, understanding terminology,
        or exploring possibilities—but it cannot examine them, know their full history, or make clinical
        decisions. The appropriate endpoint for serious health concerns is always a qualified professional.
      </p>

      <h4>2. Prompting Matters</h4>

      <p>
        Research shows that the quality of AI responses depends heavily on how questions are framed.
        Teaching patients basic prompting skills can improve the information they receive:
      </p>

      <ul>
        <li>Include relevant personal context (age, relevant medical history, current medications)</li>
        <li>Ask the AI to "act as a clinician" or "explain as if to a patient"</li>
        <li>Ask one focused question at a time rather than multiple questions</li>
        <li>Request that the AI explain its reasoning</li>
        <li>Ask for counterarguments or alternative explanations</li>
      </ul>

      <h4>3. Privacy Isn't Guaranteed</h4>

      <p>
        Consumer AI tools are not HIPAA-compliant. Data entered into ChatGPT, Claude, or similar tools
        goes to companies that may use it for training or other purposes. Patients should avoid including
        identifying information (name, date of birth, Social Security number) in health queries.
      </p>

      <h4>4. AI Can Hallucinate</h4>

      <p>
        Large language models can generate confident-sounding but entirely fabricated information. Patients
        should be especially skeptical of specific claims (drug interactions, dosages, treatment protocols)
        and verify anything important with a professional or authoritative medical source.
      </p>

      <h4>5. Emergency Symptoms Require Emergency Care</h4>

      <p>
        AI is not appropriate for evaluating urgent symptoms. Patients should understand that chest pain,
        severe shortness of breath, neurological changes, or other potentially serious symptoms require
        immediate professional evaluation—not an AI conversation.
      </p>

      <h3>Practical Tips for Patient AI Use</h3>

      <p>Consider providing patients with these concrete strategies:</p>

      <div class="callout callout-tip">
        <div class="callout-title">Better Ways to Use AI for Health Information</div>
        <ul class="mb-0">
          <li><strong>Understanding diagnoses:</strong> "Explain [condition] in simple terms—what causes it, how it's treated, and what to expect."</li>
          <li><strong>Preparing for appointments:</strong> "What questions should I ask my doctor about [condition/medication/test]?"</li>
          <li><strong>Understanding lab results:</strong> "What does a [specific test] measure and what do high/low values generally indicate?"</li>
          <li><strong>Medication information:</strong> "What are common side effects of [medication] and when should I be concerned?"</li>
          <li><strong>Learning terminology:</strong> "What does [medical term] mean in plain language?"</li>
        </ul>
      </div>

      <div class="callout callout-warning">
        <div class="callout-title">Riskier Ways to Use AI (Proceed With Caution)</div>
        <ul class="mb-0">
          <li>Self-diagnosing based on symptom descriptions</li>
          <li>Making treatment decisions without professional input</li>
          <li>Relying on AI for medication dosing or interactions</li>
          <li>Using AI during potential emergencies</li>
          <li>Trusting AI recommendations over clinical advice</li>
        </ul>
      </div>

      <h3>The "Verify Big Claims" Rule</h3>

      <p>
        As one physician advises: "If it tells you something that you think is really big, verify it with
        an expert human." This simple heuristic captures much of what patients need to know. AI can be useful
        for routine questions and background information. For anything significant—a worrisome symptom,
        a treatment decision, a major diagnosis—professional verification is essential.
      </p>

      <hr>

      <!-- Part 5: Creating AI Tools for Patients -->
      <h2>Part 5: Creating AI Tools for Your Patients</h2>

      <p>
        Rather than hoping patients use general-purpose AI wisely, you can create custom AI tools
        tailored to your practice and patients. This gives you control over the information patients
        receive while meeting them where they are. No coding required—just the skills you've already
        developed in this course.
      </p>

      <h3>Why Create Patient-Facing AI Tools?</h3>

      <ul>
        <li><strong>Control the information:</strong> Your custom GPT can only reference your approved materials, protocols, and guidance</li>
        <li><strong>Reduce after-hours burden:</strong> Patients get immediate answers to routine questions without calling the on-call line</li>
        <li><strong>Consistent messaging:</strong> Every patient gets the same accurate information, in your voice</li>
        <li><strong>Extend your care:</strong> Patients feel supported between visits</li>
        <li><strong>Triage appropriately:</strong> Build in guidance about when to seek urgent care vs. wait for an appointment</li>
      </ul>

      <h3>Ideas for Patient-Facing Custom GPTs</h3>

      <div class="example-card">
        <div class="example-header">
          <span class="example-number">1</span>
          <h4 class="example-title">Post-Procedure Care Guide</h4>
        </div>
        <p>Upload your post-operative instructions and create a GPT that answers patient questions about recovery:</p>
        <ul>
          <li>"Is this amount of bruising normal after day 3?"</li>
          <li>"Can I shower yet?"</li>
          <li>"When should I be concerned about pain levels?"</li>
        </ul>
        <p><strong>Key instruction:</strong> "Always recommend calling the office or going to the ER for fever over 101°F, signs of infection, or uncontrolled pain."</p>
      </div>

      <div class="example-card">
        <div class="example-header">
          <span class="example-number">2</span>
          <h4 class="example-title">New Diagnosis Companion</h4>
        </div>
        <p>For patients newly diagnosed with chronic conditions (diabetes, hypertension, asthma), create a GPT loaded with your practice's educational materials:</p>
        <ul>
          <li>Answers questions about the condition in plain language</li>
          <li>Explains medications you commonly prescribe</li>
          <li>Provides lifestyle guidance consistent with your recommendations</li>
          <li>Generates questions for patients to bring to their next visit</li>
        </ul>
      </div>

      <div class="example-card">
        <div class="example-header">
          <span class="example-number">3</span>
          <h4 class="example-title">Appointment Preparation Assistant</h4>
        </div>
        <p>Help patients arrive prepared:</p>
        <ul>
          <li>"What should I bring to my first visit?"</li>
          <li>"How do I describe my symptoms clearly?"</li>
          <li>"What questions should I ask about my test results?"</li>
        </ul>
        <p>This improves visit efficiency and patient satisfaction.</p>
      </div>

      <h3>How to Build It</h3>

      <p>
        If you've read the <a href="everyday-ai.html">Everyday Ways to Use AI</a> topic, you already
        have the skills. The process is identical:
      </p>

      <ol>
        <li><strong>Choose your platform:</strong> Custom GPTs (ChatGPT Plus) or Claude Projects work well. Custom GPTs can be shared via link.</li>
        <li><strong>Write clear instructions:</strong> Define the GPT's role, what it should and shouldn't discuss, and when to recommend professional care.</li>
        <li><strong>Upload your materials:</strong> Patient handouts, FAQs, post-procedure instructions, condition guides.</li>
        <li><strong>Test thoroughly:</strong> Try to break it. Ask questions that should trigger safety warnings.</li>
        <li><strong>Share with patients:</strong> Provide the link at checkout, in follow-up emails, or on your patient portal.</li>
      </ol>

      <div class="callout callout-warning">
        <div class="callout-title">Important Considerations</div>
        <ul class="mb-0">
          <li><strong>Not for emergencies:</strong> Make clear in your instructions that emergencies require 911 or the ER, not a chatbot</li>
          <li><strong>Not a replacement for care:</strong> The GPT should encourage—not replace—communication with your team</li>
          <li><strong>Privacy:</strong> Standard ChatGPT isn't HIPAA-compliant. Instruct patients not to enter identifying information. For HIPAA-compliant options, consider <a href="https://bastiongpt.com/" target="_blank">BastionGPT</a> or enterprise solutions</li>
          <li><strong>Review and update:</strong> Medical information changes. Schedule regular reviews of your GPT's knowledge base</li>
        </ul>
      </div>

      <div class="callout callout-tip">
        <div class="callout-title">Start Simple</div>
        <p class="mb-0">
          You don't need to build something comprehensive. Start with a single use case—like post-procedure
          questions for one common procedure—and expand from there. A narrow, well-designed tool is more
          useful than a broad, mediocre one.
        </p>
      </div>

      <hr>

      <!-- Part 6 -->
      <h2>Part 6: Special Populations and Considerations</h2>

      <h3>Equity Considerations</h3>

      <p>
        The intersection of AI health tools and healthcare disparities deserves attention. AI use both
        reflects and potentially exacerbates existing inequities.
      </p>

      <p>
        The finding that AI health tool users have lower educational attainment and rely more on emergency/urgent
        care settings suggests these tools may serve as a workaround for patients with less access to traditional
        primary care. According to the American Hospital Association, over 26% of Medicare beneficiaries lack
        home computer or smartphone access, and more than 22% of rural Americans lack adequate broadband.
      </p>

      <p>
        AI systems are trained on data that reflects historical patterns—including healthcare disparities.
        Populations that have been historically marginalized are underrepresented in training datasets,
        potentially leading to less accurate or relevant information for these patients.
      </p>

      <h3>Pediatric and Adolescent Considerations</h3>

      <p>Parents researching their children's health present unique dynamics:</p>

      <ul>
        <li><strong>Developmental information:</strong> Parents frequently query AI about milestones, autism spectrum features, ADHD symptoms. AI may provide generic checklists without context about normal variation.</li>
        <li><strong>Adolescent self-diagnosis:</strong> Teenagers increasingly use AI to research mental health conditions and physical symptoms. This can represent healthy self-awareness or lead to premature self-labeling.</li>
        <li><strong>Privacy for adolescents:</strong> Teens may use AI for health questions they're uncomfortable asking parents or physicians. Consider asking adolescents directly about AI health use in confidential portions of visits.</li>
      </ul>

      <h3>Mental Health Specific Concerns</h3>

      <p>AI use for mental health information deserves particular attention:</p>

      <ul>
        <li>AI tools may underestimate suicide risk, with some studies showing AI failing to appropriately escalate urgent safety concerns</li>
        <li>Self-diagnosis of personality disorders or complex trauma can be harmful without professional assessment</li>
        <li>AI-generated coping strategies may be generic and miss patient-specific contraindications</li>
        <li>Patients in crisis should not be relying on AI chatbots—ensure patients know crisis resources</li>
      </ul>

      <h3>Chronic Disease Management</h3>

      <p>
        Patients with chronic conditions present particular opportunities and risks. These patients have
        ongoing need for information and may make daily self-management decisions.
      </p>

      <p><strong>Guidance for chronic disease patients using AI:</strong></p>

      <ul>
        <li>Use AI for understanding and tracking, not for treatment changes</li>
        <li>Bring AI-generated questions to your clinical team</li>
        <li>Never adjust insulin, anticoagulation, or other high-risk medications based on AI advice</li>
        <li>Use AI to help communicate symptoms accurately to your healthcare team</li>
      </ul>

      <hr>

      <!-- Part 6 -->
      <h2>Part 6: The Opportunity</h2>

      <p>Patient AI use isn't going away. The question for clinicians is whether to view it as an obstacle or an opening.</p>

      <p>
        <em>Patients who consult AI are engaged in their health.</em> They're taking initiative, seeking
        information, and thinking about their conditions between visits. This is exactly what we want from
        patients—active participation in their healthcare.
      </p>

      <p>
        <em>AI can surface questions patients wouldn't otherwise ask.</em> When a patient brings an AI-generated
        list of considerations, they may raise issues that wouldn't have come up in a routine visit.
      </p>

      <p>
        <em>Teaching AI literacy is a form of patient education.</em> Helping patients use AI tools effectively
        reinforces critical thinking about health information generally.
      </p>

      <p>
        <em>Transparency about AI builds trust.</em> When clinicians openly discuss AI's strengths and
        limitations—rather than dismissing it—they demonstrate honesty and respect for patients' autonomy.
      </p>

      <h3>Building AI Literacy Into Your Practice</h3>

      <p>Consider how you might systematically address AI use:</p>

      <ul>
        <li><strong>Intake forms:</strong> Add a question about what resources patients have consulted, including AI tools</li>
        <li><strong>Waiting room materials:</strong> Provide guidance on using AI tools effectively and their limitations</li>
        <li><strong>Visit routine:</strong> Normalize asking about AI use as part of history-taking</li>
        <li><strong>After-visit materials:</strong> Recommend trusted resources as alternatives to unvetted AI queries</li>
        <li><strong>Staff training:</strong> Ensure all team members can respond to patient questions consistently</li>
      </ul>

      <h3>Documentation Considerations</h3>

      <p>When patient AI use is relevant to clinical decision-making, consider documenting it:</p>

      <ul>
        <li><strong>Note AI-influenced concerns:</strong> "Patient expressed concern about [condition] based on AI chatbot research. Discussed differential diagnosis and clinical reasoning."</li>
        <li><strong>Document education provided:</strong> "Discussed appropriate use of AI health tools, including limitations."</li>
        <li><strong>Record disagreements:</strong> "Patient preferred alternative approach based on AI recommendation. Discussed risks and benefits."</li>
      </ul>

      <hr>

      <!-- Quick Reference -->
      <h2>Quick Reference: When Patients Bring AI to the Visit</h2>

      <table class="comparison-table">
        <thead>
          <tr>
            <th>Scenario</th>
            <th>Approach</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Patient is prepared and informed</strong></td>
            <td>Acknowledge their preparation; use as foundation for discussion; identify areas needing clarification</td>
          </tr>
          <tr>
            <td><strong>Patient is anxious about AI findings</strong></td>
            <td>Validate concern; explain probability vs. possibility; show exam findings that provide reassurance; create safety net</td>
          </tr>
          <tr>
            <td><strong>AI recommendation differs from yours</strong></td>
            <td>Acknowledge AI logic; explain your additional data (exam, history); make reasoning transparent; invite questions</td>
          </tr>
          <tr>
            <td><strong>AI raised a valid point</strong></td>
            <td>Acknowledge openly; investigate if warranted; model appropriate use of AI as supplement to clinical judgment</td>
          </tr>
          <tr>
            <td><strong>Patient challenging your recommendation</strong></td>
            <td>Stay curious not defensive; explore what the AI said; explain clinical reasoning; respect patient autonomy while ensuring informed decision</td>
          </tr>
        </tbody>
      </table>

      <h3>Key Messages for Patients</h3>

      <ol>
        <li>AI is a starting point, not a diagnosis.</li>
        <li>Better prompts yield better information.</li>
        <li>Your data isn't private—avoid identifying information.</li>
        <li>AI can hallucinate—verify important claims.</li>
        <li>Emergency symptoms need emergency care, not AI.</li>
      </ol>

      <div class="callout callout-tip">
        <div class="callout-title">The Bottom Line</div>
        <p class="mb-0">
          Patients who consult AI are engaged in their health. Meet them where they are, help them use
          these tools wisely, and leverage their research as an opportunity for deeper conversation and better care.
        </p>
      </div>

      <hr>

      <!-- Resources -->
      <h2>Resources for Further Learning</h2>

      <h3>Essential Reading</h3>

      <div class="reading-list">
        <div class="reading-item">
          <div class="reading-type"><i data-lucide="file-text"></i></div>
          <div class="reading-content">
            <div class="reading-title">
              <a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2804309" target="_blank">"Comparing Physician and AI Chatbot Responses to Patient Questions"</a>
            </div>
            <div class="reading-meta">JAMA Internal Medicine, 2023 · The landmark study showing ChatGPT responses were preferred 79% of the time and rated 9.8x more empathetic than physician responses.</div>
          </div>
        </div>

        <div class="reading-item">
          <div class="reading-type"><i data-lucide="file-text"></i></div>
          <div class="reading-content">
            <div class="reading-title">
              <a href="https://www.statnews.com/2025/10/29/chatbots-doctors-guide-medical-appointments-questions/" target="_blank">"Doctors Need to Ask Patients About Chatbots"</a>
            </div>
            <div class="reading-meta">STAT News, October 2025 · Why "tell me about your use of chatbots" should become a standard question at appointments.</div>
          </div>
        </div>

        <div class="reading-item">
          <div class="reading-type"><i data-lucide="file-text"></i></div>
          <div class="reading-content">
            <div class="reading-title">
              <a href="https://www.statnews.com/2025/09/08/chatbots-patients-diagnosis-dr-google-story/" target="_blank">"The Right—and Wrong—Ways for Patients to Use Chatbots"</a>
            </div>
            <div class="reading-meta">STAT News, September 2025 · How chatbots are changing patient narratives: "patients aren't just arriving with facts—they're arriving with shaped, rehearsed stories."</div>
          </div>
        </div>

        <div class="reading-item">
          <div class="reading-type"><i data-lucide="file-text"></i></div>
          <div class="reading-content">
            <div class="reading-title">
              <a href="https://www.mountsinai.org/about/newsroom/2025/ai-chatbots-can-run-with-medical-misinformation-study-finds-highlighting-the-need-for-stronger-safeguards" target="_blank">"AI Chatbots Can Run With Medical Misinformation"</a>
            </div>
            <div class="reading-meta">Mount Sinai, 2025 · Study finding AI chatbots are highly vulnerable to repeating and elaborating on false medical information.</div>
          </div>
        </div>

        <div class="reading-item">
          <div class="reading-type"><i data-lucide="file-text"></i></div>
          <div class="reading-content">
            <div class="reading-title">
              <a href="https://www.washingtonpost.com/opinions/2025/10/14/ai-chatbots-health-care-diagnoses/" target="_blank">"Guidance on Using AI Chatbots for Your Health Issues"</a>
            </div>
            <div class="reading-meta">Washington Post, October 2025 · Patient-focused guidance you can share with your patients.</div>
          </div>
        </div>
      </div>

      <h3>Podcasts</h3>

      <div class="reading-list">
        <div class="reading-item">
          <div class="reading-type"><i data-lucide="headphones"></i></div>
          <div class="reading-content">
            <div class="reading-title">
              <a href="https://ai-podcast.nejm.org/" target="_blank">NEJM AI Grand Rounds</a>
            </div>
            <div class="reading-meta">Explores why empathy—not memorization—may become the most valuable clinical skill in the AI era. Episodes feature discussions about patient-AI interactions and the future of clinical conversations.</div>
          </div>
        </div>

        <div class="reading-item">
          <div class="reading-type"><i data-lucide="headphones"></i></div>
          <div class="reading-content">
            <div class="reading-title">
              <a href="https://med.stanford.edu/health-compass-podcast/ai-health-care.html" target="_blank">Stanford Health Compass: AI & Health Care</a>
            </div>
            <div class="reading-meta">Jonathan Chen, MD, PhD and Michael Pfeffer discuss how AI is changing physician-patient interactions, including research on chatbot-assisted clinical decisions.</div>
          </div>
        </div>

        <div class="reading-item">
          <div class="reading-type"><i data-lucide="headphones"></i></div>
          <div class="reading-content">
            <div class="reading-title">
              <a href="https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-the-reality-of-generative-ai-in-the-clinic/" target="_blank">"The AI Revolution in Medicine, Revisited"</a>
            </div>
            <div class="reading-meta">Microsoft Research Podcast · Dr. Murray from UCSF discusses AI's integration into clinical workflows, including patient messaging and empathetic communication.</div>
          </div>
        </div>

        <div class="reading-item">
          <div class="reading-type"><i data-lucide="headphones"></i></div>
          <div class="reading-content">
            <div class="reading-title">
              <a href="https://podcasts.apple.com/us/podcast/the-heart-of-healthcare-a-digital-health-podcast/id1575404727" target="_blank">The Heart of Healthcare</a>
            </div>
            <div class="reading-meta">Digital health podcast exploring how AI is reshaping doctor-patient interactions and the future of healthcare communication.</div>
          </div>
        </div>
      </div>

      <h3>For Your Patients</h3>

      <div class="reading-list">
        <div class="reading-item">
          <div class="reading-type"><i data-lucide="users"></i></div>
          <div class="reading-content">
            <div class="reading-title">
              <a href="https://medlineplus.gov/" target="_blank">MedlinePlus</a>
            </div>
            <div class="reading-meta">NIH's trusted patient health information—recommend as a verification source for AI-generated health claims.</div>
          </div>
        </div>

        <div class="reading-item">
          <div class="reading-type"><i data-lucide="users"></i></div>
          <div class="reading-content">
            <div class="reading-title">
              <a href="https://www.opennotes.org/" target="_blank">OpenNotes</a>
            </div>
            <div class="reading-meta">Research and resources on patient access to clinical notes—context for discussions about AI interpretation of medical records.</div>
          </div>
        </div>
      </div>

      <h3>Building Patient-Facing AI Tools</h3>

      <div class="reading-list">
        <div class="reading-item">
          <div class="reading-type"><i data-lucide="wrench"></i></div>
          <div class="reading-content">
            <div class="reading-title">
              <a href="https://pubmed.ncbi.nlm.nih.gov/38285894/" target="_blank">"Twelve Tips on Creating Custom GPTs for Health Professions Education"</a>
            </div>
            <div class="reading-meta">PubMed, 2024 · Practical guide to building custom GPTs—applies directly to patient-facing tools.</div>
          </div>
        </div>

        <div class="reading-item">
          <div class="reading-type"><i data-lucide="wrench"></i></div>
          <div class="reading-content">
            <div class="reading-title">
              <a href="https://bastiongpt.com/" target="_blank">BastionGPT</a>
            </div>
            <div class="reading-meta">HIPAA-compliant medical GPT platform for practices needing compliant patient-facing AI tools.</div>
          </div>
        </div>
      </div>

      <hr>

      <!-- Patient-Facing AI Tools -->
      <h2>Patient-Facing AI Health Tools</h2>

      <p>
        Beyond general-purpose chatbots like ChatGPT and Gemini, a growing category of purpose-built
        AI health tools is emerging specifically for patient use. These tools aim to provide more
        clinically appropriate responses than general AI, often with built-in safeguards, physician
        oversight, or connections to care. Patients may mention using these tools, and it's worth
        understanding what they offer.
      </p>

      <div class="callout callout-warning">
        <div class="callout-title">Important Context</div>
        <p class="mb-0">
          These tools are not endorsements. The patient-facing AI health space is evolving rapidly,
          with new entrants and changing features. Encourage patients to discuss any AI-derived
          guidance with their healthcare team, regardless of the source.
        </p>
      </div>

      <h3>Purpose-Built Patient Health AI</h3>

      <div class="table-responsive">
        <table class="reference-table">
          <thead>
            <tr>
              <th>Tool</th>
              <th>What It Does</th>
              <th>Key Features</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong><a href="https://www.meetvirginia.app/" target="_blank">Meet Virginia</a></strong></td>
              <td>AI tool for appointment preparation</td>
              <td>
                <ul style="margin: 0; padding-left: 1.2rem;">
                  <li>Helps patients prepare and organize information before visits</li>
                  <li>Structures symptoms, concerns, and questions for appointments</li>
                  <li>Designed to make clinical encounters more productive</li>
                </ul>
              </td>
            </tr>
            <tr>
              <td><strong><a href="https://www.counselhealth.com/" target="_blank">Counsel Health</a></strong></td>
              <td>AI chatbot with on-demand physician access</td>
              <td>
                <ul style="margin: 0; padding-left: 1.2rem;">
                  <li>Medical-grade AI collects history and provides initial guidance</li>
                  <li>Escalates to live telehealth with a physician when needed ($29)</li>
                  <li>Emergency detection for urgent symptoms</li>
                  <li>HIPAA and SOC 2 compliant</li>
                  <li>Can connect medical records for personalized responses</li>
                </ul>
              </td>
            </tr>
            <tr>
              <td><strong><a href="https://about.mydoctorfriend.ai/" target="_blank">My Doctor Friend</a></strong></td>
              <td>AI health copilot for symptom tracking and guidance</td>
              <td>
                <ul style="margin: 0; padding-left: 1.2rem;">
                  <li>Tracks symptoms from onset through recovery</li>
                  <li>Adjusts guidance as symptoms evolve</li>
                  <li>Helps determine when to seek care</li>
                  <li>iOS app in beta</li>
                </ul>
              </td>
            </tr>
            <tr>
              <td><strong><a href="https://ada.com/" target="_blank">Ada Health</a></strong></td>
              <td>Established AI symptom assessment app</td>
              <td>
                <ul style="margin: 0; padding-left: 1.2rem;">
                  <li>Conversational symptom checker</li>
                  <li>Provides possible conditions ranked by likelihood</li>
                  <li>Widely used (millions of downloads)</li>
                  <li>Integrates with some health systems</li>
                </ul>
              </td>
            </tr>
            <tr>
              <td><strong><a href="https://ubiehealth.com/" target="_blank">Ubie</a></strong></td>
              <td>AI symptom checker with care navigation</td>
              <td>
                <ul style="margin: 0; padding-left: 1.2rem;">
                  <li>Considers age, medical history, and lifestyle factors</li>
                  <li>Generates symptom summary for doctor visits</li>
                  <li>71.6% top-10 diagnostic accuracy in testing</li>
                </ul>
              </td>
            </tr>
            <tr>
              <td><strong><a href="https://docus.ai/" target="_blank">Docus</a></strong></td>
              <td>AI health platform with specialist access</td>
              <td>
                <ul style="margin: 0; padding-left: 1.2rem;">
                  <li>AI symptom checker and lab interpretation</li>
                  <li>Option for second opinions from specialists</li>
                  <li>Health report generation</li>
                </ul>
              </td>
            </tr>
          </tbody>
        </table>
      </div>

      <h3>What Makes These Different From ChatGPT?</h3>

      <p>Purpose-built health AI tools typically offer several advantages over general-purpose chatbots:</p>

      <ul>
        <li><strong>Clinical guardrails:</strong> Built-in protocols for recognizing emergencies and escalating appropriately</li>
        <li><strong>Structured symptom collection:</strong> Systematic approaches that gather relevant clinical information</li>
        <li><strong>Physician oversight:</strong> Some tools have physicians reviewing outputs or available for escalation</li>
        <li><strong>Healthcare compliance:</strong> Many are HIPAA-compliant, unlike consumer chatbots</li>
        <li><strong>Care connections:</strong> Pathways to telehealth, appointments, or specialist consultations</li>
      </ul>

      <p>
        That said, these tools still share core limitations with all AI: they cannot perform physical
        examinations, they may miss atypical presentations, and they work best as a complement to—not
        replacement for—professional care.
      </p>

      <h3>Talking With Patients About These Tools</h3>

      <p>When patients mention using purpose-built health AI:</p>

      <ul>
        <li><strong>Ask what they learned:</strong> "What did the app tell you? Let's see how that fits with what I'm finding."</li>
        <li><strong>Validate appropriate use:</strong> "Using that to organize your symptoms before coming in was helpful."</li>
        <li><strong>Clarify limitations:</strong> "These tools are good at generating possibilities, but they can't examine you or know your full history."</li>
        <li><strong>Distinguish from diagnosis:</strong> "The app gave you a list of possibilities—let me help narrow that down based on your exam."</li>
      </ul>

      <div class="callout callout-tip">
        <div class="callout-title">The Emerging Landscape</div>
        <p class="mb-0">
          This space is evolving rapidly. New tools launch regularly, existing tools add features, and
          some will inevitably disappear. The core principles remain constant: AI tools can help patients
          prepare, understand, and engage—but they work best when integrated with, not substituted for,
          professional clinical care.
        </p>
      </div>

      <!-- Learning Objectives -->
      <div class="learning-objectives">
        <h3>Learning Objectives</h3>
        <p>After completing this topic, you should be able to:</p>
        <ul>
          <li>Recognize patterns of patient AI use in clinical encounters</li>
          <li>Apply communication strategies for partnering with AI-using patients</li>
          <li>Address discrepancies between AI-generated information and clinical recommendations</li>
          <li>Teach patients to use AI health tools more effectively and safely</li>
          <li>Identify special considerations for vulnerable populations</li>
          <li>Transform patient AI use into opportunities for engagement and education</li>
        </ul>
      </div>

      <!-- Page Navigation -->
      <nav class="page-nav">
        <a href="everyday-ai.html" class="page-nav-link prev">
          <span class="page-nav-arrow"><i data-lucide="arrow-left"></i></span>
          <div>
            <span class="page-nav-label">Previous</span>
            <span class="page-nav-title">Everyday Ways to Use AI</span>
          </div>
        </a>
        <a href="vibe-coding.html" class="page-nav-link next">
          <span class="page-nav-arrow"><i data-lucide="arrow-right"></i></span>
          <div>
            <span class="page-nav-label">Next</span>
            <span class="page-nav-title">Vibe Coding</span>
          </div>
        </a>
      </nav>

    </article>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      <div><strong>AI 101</strong> · A Self-Paced Guide to AI in Medicine</div>
      <div>v1.0 · 2025</div>
    </div>
  </footer>

  <script>
    lucide.createIcons();
    const navToggle = document.querySelector('.nav-toggle');
    const navLinks = document.querySelector('.nav-links');
    if (navToggle) {
      navToggle.addEventListener('click', () => {
        navLinks.classList.toggle('nav-open');
        navToggle.classList.toggle('nav-toggle-active');
      });
    }
  </script>

  <script src="https://studio.pickaxe.co/api/embed/bundle.js" defer></script>

</body>
</html>
