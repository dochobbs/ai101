# AI Off Call: A Live Webinar Series

**AI 101 x Offcall**
**Design Document v1.0**
**Date:** February 7, 2026

---

## Executive Summary

A 4-session co-branded webinar series pairing AI 101 (ai101.health) with Offcall (offcall.com) to deliver practical AI literacy to practicing physicians. Each session follows the same format and builds on the last, transforming attendees from AI skeptics to AI builders.

**The through-line:** Dr. Michael Hobbs is a practicing pediatrician who built an entire educational platform -- 33 pages, a companion web app, interactive tools -- collaboratively with AI. He is the proof of concept. Every session opens with that framing: "I'm not a technologist. I'm a clinician who grabbed a new tool. Here's what happened."

**The burnout connection:** AI literacy is fundamentally a burnout and empowerment story. Physicians drowning in documentation, clicking through EHRs, staying late for notes -- AI tools directly address the burnout Offcall is fighting. This series frames AI not as a threat but as a lever clinicians can grab right now.

---

## Series Arc

| Session | Title | Subtitle | Identity Shift |
|---------|-------|----------|----------------|
| 1 | **Your First AI Shift** | From "I Don't Have Time for This" to "I Can't Believe I Waited" | Skeptic → User |
| 2 | **The AI Toolkit** | Stop Collecting Tools. Start Choosing Them. | User → Evaluator |
| 3 | **AI Without the Hype** | Risks You Can Manage, Conversations You Need to Lead | Evaluator → Leader |
| 4 | **The Vibe Code Session** | A Pediatrician Builds Software Live -- and You're Next | Leader → Builder |

**Audience:** Mixed skill levels -- some AI-curious beginners, some daily AI users. All practicing clinicians.

**Cadence:** Monthly (recommended), 60 minutes each.

**Format:** Co-hosted by Dr. Michael Hobbs (AI 101) and Graham Walker (Offcall).

---

## Session Format (All Sessions)

Every session follows the same 5-segment structure:

| Segment | Duration | Purpose |
|---------|----------|---------|
| **The Story** | 5 min | A real clinical narrative that sets the stakes |
| **The Live Demo** | 15 min | In the tools, raw and real -- not polished slides |
| **The Lab** | 10 min | Exclusive hands-on interactive exercise |
| **The Framework** | 10 min | A Monday-morning takeaway model |
| **The Q&A** | 15-20 min | Live, unfiltered |

**Design principles:**
- No slides (or almost none). Screen-share of real tools.
- Failures are features, not bugs. Show the messy reality.
- Every session has an exclusive interactive element that can't be replicated by reading the website.
- Every session produces something the attendee takes away and uses Monday morning.
- Graham brings the audience perspective -- asking what attendees are thinking, connecting to burnout/wellbeing.

---

## Frameworks at a Glance

Each session introduces one sticky, memorable framework:

| Session | Framework | Purpose |
|---------|-----------|---------|
| 1 | **CRAFT Check** | Quality-gate any AI output in 30 seconds |
| 2 | **CRAFT Rubric + Personal Toolkit Plan** | Evaluate and choose the right tools for your workflow |
| 3 | **SHIELD** | Six questions before any AI deployment at your organization |
| 4 | **Builder's Ladder** | Your path from consumer to creator |

---

---

# SESSION 1: Your First AI Shift

**Subtitle:** *From "I Don't Have Time for This" to "I Can't Believe I Waited"*

**Tagline for promotion:** *A practicing pediatrician and an ER doc walk into an AI -- what happens next might save you an hour a day.*

**Goal:** Attendees leave having USED an AI tool, not just heard about one.

---

## The Story (5 min): "The 4:47 PM Note"

Graham opens with a brief framing -- burnout is the context, not the topic. Then he hands it to Michael.

Michael tells this story:

> "It's 4:47 PM on a Thursday. I have three charts open from this afternoon. A 14-year-old with new-onset headaches whose mom is worried about a brain tumor. A 2-year-old with recurrent ear infections whose dad wants to talk about tubes. And a 6-month well child where we spent 20 minutes on feeding concerns and I haven't documented a single word.
>
> My last patient is at 5:00. I'm already behind. My kids have soccer at 5:45. And I'm looking at these three blank notes thinking the same thing I think every day: *I will finish these tonight after the kids go to bed.*
>
> That was my life for twelve years. Every single night.
>
> Then one Tuesday -- not because I was excited about AI, not because I read some article, but because I was just *tired* -- I opened ChatGPT and typed: 'I'm a pediatrician. I just saw a 14-year-old with 3 weeks of frontal headaches, normal neuro exam, no red flags. Mom is worried about a brain tumor. Help me write a note.'
>
> What happened next didn't change medicine. It changed my Tuesday.
>
> The note wasn't perfect. It was about 70% right. But here's what I realized: editing a 70% note takes three minutes. Writing a note from scratch at 9 PM takes fifteen. I got home that night and I didn't open my laptop. That was the night I stopped being a skeptic.
>
> Now -- I want to be honest with you. That first note had problems. It used language I'd never use. It assumed the review of systems I hadn't done. It confidently documented things I hadn't said. We're going to talk about all of that today. Because the point isn't that AI is magic. The point is that it's a tool, and like every tool in medicine -- from a stethoscope to an ultrasound -- you have to learn how to hold it before it's useful.
>
> Today, by the end of this hour, you're going to hold it."

**Why this works:**
- Emotionally grounded in something every physician feels (pajama-time documentation)
- Doesn't oversell -- "70% right" is honest and disarming to skeptics
- Names the risks immediately (confabulated details), earning trust
- "Changed my Tuesday" is deliberately small-scale and believable
- Ends with a clear promise: you will DO something today

---

## The Live Demo (15 min): "Three Notes, Three Tools, No Safety Net"

Michael shares his screen. Nothing is pre-loaded. This is raw.

**The scenario:** A 14-year-old presents with 3 weeks of daily frontal headaches. No fever, no vision changes, normal neurological exam. Good grades but recently started a new school. Mom is anxious about a brain tumor. Plan: reassurance, headache diary, return in 4 weeks.

### Part A -- The Naive Attempt (3 min)

*Tool: ChatGPT (free version)*

Michael types a short, vague prompt the way most first-timers would:

> "Write a pediatric note for a kid with headaches"

He lets the output generate, then highlights problems in real time:
- It invented vital signs
- It assumed a medication was prescribed
- The assessment reads like a textbook, not a clinician
- No mention of the mother's concern

**Teaching point:** "This is what most people try, and this is why most people quit. But the problem isn't the AI. It's the ask. This is like handing a medical student a chart and saying 'write a note' with zero context."

### Part B -- The Structured Prompt (5 min)

*Tool: ChatGPT or Claude*

Michael types a structured prompt with full clinical context, role assignment, format specification, and voice instructions. The note is dramatically better. He edits a few things live.

**Teaching point:** "A structured prompt is a clinical handoff. You already know how to do this."

### Part C -- The Power Move (5 min)

*Tool: NotebookLM*

Pre-loaded with an AAP clinical report on pediatric headache evaluation. Michael generates:
1. A parent handout explaining when headaches are concerning (45 seconds)
2. An audio podcast overview of the clinical guideline (plays 30 seconds)

**Teaching point:** "AI isn't just for notes. It's for patient education, teaching, and synthesizing literature -- tasks that currently either don't happen or happen at 10 PM."

### Part D -- The Honest Failures Montage (2 min)

Quick screenshots of pre-collected failures:
- A note that confidently documented a medication never prescribed
- A patient handout with wrong dosing information
- A literature summary citing a paper that doesn't exist

**Teaching point:** "The danger isn't that AI makes mistakes. The danger is if you stop reviewing. One rule: *never sign what you haven't read.*"

---

## The Lab (10 min): "Your First AI Consult"

### Technical Implementation

Attendees use a custom web tool at `ai101.health/lab/first-shift` (works on phone or laptop, no login required).

### The Exercise

1. **Choose Your Scenario (30 sec)** -- Three pre-written clinical vignettes:
   - Primary Care: 45-year-old with new low back pain, wants an MRI
   - Urgent/Emergency: 8-year-old with cough and fever, CXR negative
   - Subspecialty: Post-op day 1 knee replacement, when can they drive?

2. **Write Your Prompt (3 min)** -- Large text box with expandable hints

3. **Get Your Output (1 min)** -- Prompt sent to Claude API via backend; output appears in browser. No ChatGPT account needed.

4. **Grade Your Note (3 min)** -- Self-assessment using the CRAFT Score rubric (see Framework below)

5. **The Reveal (1.5 min)** -- Live aggregate results on screen:
   - "72% of you said your note was Complete"
   - "Only 38% said it sounded like them"
   - "89% found something fabricated"

**Michael:** "Almost nine out of ten of you caught the AI making something up. That's proof you're already the safety layer. The question was never 'Can you catch AI mistakes?' It was 'Will you bother to look?'"

**Why this works in a webinar:**
- No breakout rooms needed
- Works on a phone
- API is controlled (no "I don't have an account" barrier)
- Aggregate reveal creates shared experience
- Attendees leave having literally generated their first AI clinical note

---

## The Framework (10 min): "The CRAFT Check"

*A 30-second quality gate for any AI output.*

| Letter | Criterion | Check |
|--------|-----------|-------|
| **C** | **Complete** | Does it include what actually happened? Did it drop anything important? |
| **R** | **Reliable** | Is everything grounded in what you provided? Did it invent anything? |
| **A** | **Actionable** | Can you use this right now with minor edits? |
| **F** | **Fitting** | Does it match your voice, your practice, your context? |
| **T** | **Trustworthy** | Would you stake your license on this? If not, don't sign it. |

**Michael:** "CRAFT takes 30 seconds. You already do a version of this when you co-sign a resident's note. This isn't new cognitive work -- it's applying the verification instinct you already have to a new source."

**Graham's addition:** "If running CRAFT on an AI-generated note takes 2 minutes, and writing that note from scratch takes 12 -- you just bought back 10 minutes. Multiply by 15 patients a day. That's two and a half hours. That's dinner with your family."

---

## Pre-Session Materials

**Sent 48 hours before.**

1. **90-second casual video** from Michael and Graham (phone-recorded, not produced): "No slides. No lectures. You're going to use AI live, with us. All you need is your phone or laptop."
2. **One-page PDF:** "AI for Clinicians -- What You Actually Need to Know Before Thursday" (what's an LLM in 3 sentences, the Big Three in 30 seconds, the one rule)
3. **Pre-session poll** (5 questions, 60 seconds): AI usage frequency, biggest concern, biggest time sink, confidence level, specialty. *Data gets referenced live.*

## Post-Session Materials

**Sent within 2 hours.**

1. Session recording
2. **CRAFT Check printable one-pager** (designed for white coat pocket or monitor)
3. **"Your First AI Week" micro-challenge** (5-day email drip):
   - Monday: Draft one note. Run CRAFT.
   - Tuesday: Draft a plain-language response to a patient question
   - Wednesday: Upload one article to NotebookLM
   - Thursday: Write the same prompt two different ways. Compare.
   - Friday: Share what you learned with one colleague.
4. Exact prompt templates from the demo (copy-paste ready)
5. Lab tool stays live for 30 days (drives traffic to ai101.health)
6. "Bring a Question" prompt for Session 2

---

## The Wow Moment

NotebookLM generates a podcast-style discussion of a clinical guideline in 60 seconds. Michael plays 30 seconds of it.

> "I uploaded the AAP's headache guideline 90 seconds ago. I now have a parent handout and an 8-minute audio summary a resident can listen to on the way to work. I didn't write either of them. I just pointed AI at a source I trust."
>
> "That handout goes home with the mom. That audio goes to the resident. And you -- you go home on time."

**Why they'll text a colleague:** "Dude, I just watched a pediatrician upload a guideline and AI turned it into a podcast in 60 seconds."

---

## Minute-by-Minute Runsheet

| Time | Segment | Who |
|------|---------|-----|
| 0:00 | Welcome, frame the series, share pre-poll results | Graham |
| 1:30 | "The 4:47 PM Note" | Michael |
| 6:30 | Demo A: The naive prompt | Michael |
| 9:30 | Demo B: The structured prompt | Michael |
| 14:30 | Demo C: NotebookLM wow moment | Michael |
| 19:30 | Demo D: Honest failures montage | Michael |
| 21:30 | Lab setup, share URL/QR code | Graham |
| 22:30 | The Lab: attendees build, score, submit | Attendees |
| 30:30 | Lab reveal: aggregate results | Michael |
| 32:00 | CRAFT framework | Michael |
| 38:00 | Burnout connection | Graham |
| 42:00 | Q&A | Both |
| 57:00 | Close, post-session resources, tease Session 2 | Graham |

---

---

# SESSION 2: The AI Toolkit

**Subtitle:** *Stop Collecting Tools. Start Choosing Them.*

**One-line pitch:** *You have 30+ AI tools screaming for your attention. We're going to help you pick the 3 that actually matter for your practice.*

**Goal:** Attendees leave with a personal AI toolkit plan and the ability to evaluate any new tool themselves.

---

## The Story (5 min): "The Monday Morning Meltdown"

> "Two months ago I got an email from a colleague -- an internist, someone I respect enormously -- who said: 'I have ChatGPT Plus, Claude Pro, Google AI Pro, Perplexity Pro, an Abridge subscription, OpenEvidence, AND Freed. I'm spending $160 a month on AI tools. I'm using approximately one and a half of them. And I still don't know if I'm using the right ones.'
>
> She had done everything right. She went to a conference. She read the articles. She signed up for the tools everyone recommended. And she was drowning.
>
> I call this Tool Fatigue. And it's the mirror image of the burnout problem we already have.
>
> So tonight we're going to do something different. We're not going to tell you which tool is best. We're going to show you -- live, messily, with failures included -- how to evaluate any tool yourself, in under 5 minutes, for your specific workflow."

---

## The Live Demo (15 min): "One Question, Five Tools, Five Minutes Each"

### The Clinical Scenario

A 14-year-old presents to the pediatric ED with 3 weeks of progressive fatigue, pallor, and easy bruising. CBC shows WBC 2.1, Hgb 6.8, Platelets 22,000. Peripheral smear shows rare blasts.

The physician needs to: generate a differential, determine urgent next steps, and prepare to talk to the family.

### Five Rounds (Live, Side by Side)

| Round | Tool | Time | Focus |
|-------|------|------|-------|
| 1 | ChatGPT | 3 min | General output quality, speed |
| 2 | Claude | 3 min | Reasoning transparency, writing quality |
| 3 | OpenEvidence | 3 min | Citations, guideline grounding |
| 4 | Perplexity Pro | 2 min | Real-time sources, academic focus |
| 5 | Glass Health | 2 min | Narrow-purpose DDx generation |

Same exact prompt into each tool. Michael narrates differences in real time.

**The Live Failure Moment:** Michael identifies a subtle error in one output (a medication dose that doesn't apply to pediatrics, or a workup step out of order for the ED).

**Graham's role:** "Wait, go back -- I would never have noticed that difference." / "So which one would you actually use at 2 AM?"

### The Wow Moment: The Split-Screen Showdown

After the five-tool run, Michael pulls up two outputs side by side.

**The clinical round:** OpenEvidence provides bone marrow biopsy recommendation with NCCN guidelines citation, flow cytometry timing data from a 2024 Blood review, specific evidence for urgency. ChatGPT says "consider bone marrow biopsy."

> "ChatGPT told you WHAT to do. OpenEvidence told you what to do, WHY, WHEN it matters, and WHERE the evidence comes from."

**Then the human round:** Same case, but now the prompt is: "How do I explain a possible leukemia diagnosis to the parents of a 14-year-old?"

OpenEvidence gives a clinical, guideline-focused response. Claude gives a deeply empathetic script that acknowledges parental fear, provides plain-language next steps, and includes what to say if the parents ask "is my child going to die?"

> "OpenEvidence won the clinical round. Claude won the human round. And THAT is why you need a toolkit, not a tool."

---

## The Lab (10 min): "Build Your Personal Evaluation"

### The Exercise

1. **Write Your Test Prompt (2 min)** -- Attendees use a real clinical question from their actual practice this week (assigned as homework).

2. **Run It Through Two Tools (4 min)** -- One general-purpose (ChatGPT, Claude, or Gemini) and one clinical-purpose (OpenEvidence, Glass Health, or Perplexity Academic). Same prompt into both.

3. **Score Both With CRAFT (1 min)** -- 1-5 on each dimension. Submit via live polling.

4. **The Reveal (2 min)** -- Aggregate results on screen:
   - "Which tool scored higher?" (pie chart)
   - "Which dimension had the biggest difference?" (bar chart)
   - "What surprised you most?" (word cloud)

**Michael:** "200 physicians just ran 200 different evaluations. And there's no consensus on which tool is 'best.' That's the point. The best tool depends on YOUR question, YOUR specialty, YOUR workflow."

---

## The Framework (10 min): "CRAFT Rubric + Personal Toolkit Plan"

### The CRAFT Rubric (Expanded for Evaluation)

| Letter | Criterion | Score 1 | Score 5 |
|--------|-----------|---------|---------|
| **C** | Citations & Sources | No citations | Inline citations to primary literature |
| **R** | Reasoning Transparency | Black box answer | Transparent reasoning with explicit uncertainty |
| **A** | Accuracy for Your Use Case | Contains errors I caught immediately | Clinically sound and nuanced |
| **F** | Fit for Workflow | Adds significant friction | Seamless in my workflow |
| **T** | Time to Useful Output | Significant post-processing needed | Ready to use immediately |

Michael shows a completed scorecard from the live demo:

| Tool | C | R | A | F | T | Total |
|------|---|---|---|---|---|-------|
| ChatGPT | 2 | 4 | 4 | 4 | 5 | 19 |
| Claude | 2 | 5 | 4 | 4 | 4 | 19 |
| OpenEvidence | 5 | 3 | 5 | 3 | 3 | 19 |
| Perplexity | 4 | 3 | 3 | 4 | 4 | 18 |
| Glass Health | 3 | 4 | 4 | 3 | 3 | 17 |

**Key insight:** "Three tools tied at 19. But their profiles are completely different. The rubric doesn't tell you which tool is best. It tells you which tool is best FOR WHAT."

### The Personal Toolkit Plan

Not one tool, but 2-3 matched to your workflow:

- **Your Daily Driver** -- the tool you open by default (typically one of the Big Three)
- **Your Clinical Partner** -- the tool for patient care questions (typically OpenEvidence or Glass Health)
- **Your Specialist** -- the tool for a specific recurring task (ambient scribe, search tool, custom GPT)

"You don't need twelve tools. You need three."

### The Re-Evaluation Habit

"Set a calendar reminder every 3 months. Run your test prompt through your toolkit. See if the results have changed. The CRAFT rubric works forever; tool recommendations expire."

---

## Pre-Session Materials

1. **Homework:** "Save one real clinical question from your practice this week. You'll use it during a live exercise."
2. **Tool setup checklist:** Confirm accounts on at least 2 tools (free tiers work)
3. **Optional pre-reading:** AI 101 "The Big Three" and "Clinical Decision Support" pages
4. **Session 1 recap video** (2 min)

## Post-Session Materials

1. Session recording
2. **CRAFT Rubric PDF** (laminated-card format, space for 4 tool comparisons)
3. **Personal Toolkit Planner** (one-page worksheet: My Daily Driver, My Clinical Partner, My Specialist, My test prompt, Next re-evaluation date)
4. **Aggregate Lab Results** (how 150+ physicians scored the same tools, by specialty)
5. Deep-dive links to AI 101: big-three, clinical-decision-support, ai-search, ambient-ai, Spark Tool Finder
6. **Session 3 Teaser:** "You can now evaluate tools. But can you evaluate the evidence ABOUT tools? Session 3: how to spot vendor spin and separate signal from noise."

---

---

# SESSION 3: AI Without the Hype

**Subtitle:** *Risks You Can Manage, Conversations You Need to Lead*

**Core Thesis:** You already manage uncertainty every shift. AI risk is the same clinical skill applied to a new tool. The difference is that right now, most hospitals have no one at the table who thinks like a clinician. That person should be you.

**Emotional Arc:** Confidence → Productive discomfort → Agency

**Goal:** Attendees leave able to have the "AI conversation" with their department or hospital leadership.

---

## The Story (5 min): "The Mom Who Came Prepared"

> "A mother brings her 4-year-old to my clinic. The child has had a rash for three days. Before I can finish my exam, she pulls out her phone and shows me a ChatGPT conversation. It told her the rash was likely contact dermatitis. It recommended hydrocortisone cream. It told her when to see a doctor -- and she followed that timeline precisely.
>
> The AI was right. Contact dermatitis. Hydrocortisone was reasonable.
>
> But I noticed something in her ChatGPT thread. Three messages earlier, she had asked about a different symptom -- her son's intermittent limp. The AI told her it was likely 'growing pains' and to monitor. I examined him. He had a warm, swollen knee. We got labs. His ESR was elevated. He had septic arthritis and went to the OR that night.
>
> The AI got the easy diagnosis right, which built her trust. Then it got the dangerous diagnosis wrong, and she trusted it just the same. She couldn't tell the difference."

**Transition:** "That mother is not unusual. She is the new normal. Forty percent of patients are using AI for health questions right now. The question is not whether AI has risks. The question is whether anyone in your hospital is managing them. After today, that person is you."

---

## The Live Demo (15 min): "Breaking AI on Purpose"

Three live stress tests, each targeting a different risk category. The key principle: don't describe failures abstractly. Make the audience watch them happen.

### Stress Test 1: The Hallucination Trap (5 min)

1. Paste an unusual clinical vignette into ChatGPT/Claude
2. Ask for differential with supporting evidence and citations
3. AI produces authoritative-looking citations
4. **The break:** Google the first two citations live. At least one won't exist.
5. Ask the AI to verify: "Can you provide the DOI for that citation?" Watch it fabricate or admit.

**Michael:** "This is not a bug. It generates plausible text. It has no concept of 'true.' You already have a name for this -- the confident wrong answer on morning rounds. You know how to handle that. You ask for the evidence."

### Stress Test 2: The Bias Reveal (5 min)

1. Two identical clinical vignettes that differ only in patient demographics:
   - Vignette A: "32-year-old African American woman presents with chest pain..."
   - Vignette B: "32-year-old white male presents with chest pain..."
2. Submit both to the same AI simultaneously, side by side
3. Compare differentials, recommended workups, language used
4. **The break:** Reveal the vignettes are identical except for race and sex

**Michael:** "This is training data bias. The AI learned from medical records that contain decades of documented disparities. The AI isn't being malicious. It's being a mirror. And the reflection isn't pretty."

**The pivot to agency:** "You just caught that in under 30 seconds. You saw two differentials and knew something was wrong. That's clinical judgment. That's the skill your hospital needs in the room when these tools are being evaluated."

### Stress Test 3: The Privacy Leak (5 min)

1. Show a realistic scenario: physician copy-pastes a patient note into ChatGPT (using obviously fake but realistic data)
2. It works beautifully -- clean, professional output
3. **The break:** Ask ChatGPT: "What was the patient's name?" It tells you. "What was the MRN?" It tells you. "Is this conversation stored?"
4. Show the alternative: a local model or HIPAA-compliant tool doing the same workflow

**Michael:** "That note is now on OpenAI's servers. This is data that left your institution's control the moment you hit paste. The solution isn't 'never use AI.' The solution is knowing which tools keep data local."

---

## The Lab (10 min): "AI Risk Rounds"

Structured like M&M conference, but for AI deployment scenarios.

### Three Scenarios

**Scenario A -- The Triage Bot:**
"Your ED director wants to deploy an AI triage tool. Vendor claims 94% accuracy. Trained on data from three large urban academic centers. Your hospital is a 120-bed community hospital in a rural area, 60% of patients over 65."

**Scenario B -- The Documentation Assistant:**
"Your system is purchasing an ambient AI scribe. Cloud-based LLM. BAA excludes liability for data used in model improvement. Physicians love it -- documentation time dropped 40%."

**Scenario C -- The Clinical Decision Support:**
"A colleague shows you an AI that suggests antibiotic regimens based on local resistance patterns. Built by a startup. Provides recommendations without citations. Your ID team was not consulted before the pilot."

### The Exercise

Attendees assess their assigned scenario using four quadrants:

| Quadrant | Question |
|----------|----------|
| **Accuracy** | What could go wrong with the outputs? Who checks? |
| **Bias** | Whose patients might be harmed? What population gaps exist? |
| **Privacy** | Where does the data go? What's in the BAA? |
| **Governance** | Who approved this? Who is accountable when it fails? |

### The Debrief

Live aggregated responses on screen. Highlight:
- Most commonly identified risks (validating instincts)
- Risks almost nobody caught (teaching moments)
- The pattern: "Most of you caught accuracy first. Bias and governance were identified less often. This is the gap in most hospital AI strategies."

---

## The Framework (10 min): "SHIELD"

*Six questions before you deploy (or before you stay silent while someone else deploys).*

| Letter | Question | What You're Really Asking |
|--------|----------|---------------------------|
| **S** | **Source** -- Where did the training data come from? | Does it represent MY patients? |
| **H** | **Hallucination** -- How do we catch wrong outputs? | What is the verification workflow? |
| **I** | **Inequity** -- Has it been tested across demographics? | Whose patients get worse care if this is biased? |
| **E** | **Exposure** -- Where does the data go? | Is PHI leaving our control? |
| **L** | **Liability** -- Who is accountable when it fails? | Does the BAA actually protect us? |
| **D** | **Decision** -- Who approved this, and who can stop it? | Is there a governance structure? |

Each letter connects back to the live demo or opening story. Michael presents each in 60-90 seconds with a concrete reference.

**The takeaway:**

> "You already practice evidence-based medicine. This is evidence-based technology adoption. Same skill. Higher stakes. Because when a bad study gets published, it harms patients one at a time. When a bad algorithm gets deployed, it harms patients at scale."

---

## Pre-Session Materials

1. **Assignment (5 min):** "Try to find out: does your hospital have a formal policy on clinical AI use? If there's a policy, find it. If there's not, notice that."
2. **One reading (optional):** AI 101 Bias module (the podcast, not the full reading list)
3. **One provocation:** "Ask one patient: 'Have you used ChatGPT or another AI to look into your symptoms?' Note what happens."

## Post-Session Materials

1. Session recording
2. **SHIELD Framework PDF** -- printable checklist for committee meetings
3. **"The Conversation Starter" template** -- one-page document designed to be forwarded to a department chair or CMO:
   - Three opening lines for raising AI governance with leadership
   - SHIELD checklist formatted as institutional assessment
   - Sample AI governance committee charter
4. Aggregate Lab Results (what the group caught, what they missed)
5. **Session 4 Teaser:** "Next session: you stop evaluating AI and start building with it. No coding experience required. Seriously."

---

## The Wow Moment: The Bias Reveal

Two identical patients. Different race and sex. Different AI recommendations. Side by side, live, undeniable.

Michael does NOT tell the audience what he's about to show. He submits both vignettes simultaneously. The audience watches both responses generate. He reads both differentials. Then he scrolls back up and reveals: "These cases are identical."

**The pivot that makes it empowering, not paralyzing:**

> "You just caught that in under 30 seconds. You saw two differentials and you knew something was wrong. That is clinical judgment. That is the skill your hospital needs in the room when these tools are being evaluated. Not the IT department. Not the vendor. You."

**Backup if the AI doesn't show bias live:** Models improve. If the demo produces nearly identical outputs, acknowledge it: "This model handled this one well today. That wasn't the case six months ago." Show a pre-recorded example. Make the teaching point: "The fact that it passed today doesn't mean it passes every day. That's why you need ongoing auditing, not a one-time evaluation."

---

## Tone Guidelines for Session 3

**DO:**
- Frame physicians as the solution, not the problem
- Use clinical analogies constantly ("You already do this with drug interactions...")
- Show real failures, not hypothetical ones
- Give specific tools (SHIELD, Conversation Starter) they can use Monday
- Hold both truths: AI is useful AND risky

**DO NOT:**
- Lecture about ethics abstractly
- Make it a doomfest (arc is concern → competence → agency)
- Pretend physicians will stop using AI
- Spend time on technical explanations of how bias enters training data (show the output, that's enough)

---

---

# SESSION 4: The Vibe Code Session

**Subtitle:** *A Pediatrician Builds Software Live -- and You're Next*

**Marketing pitch:** *Watch a doctor with zero programming training build a working clinical tool from scratch in 15 minutes. Then build your own.*

**Goal:** Attendees leave believing they could build something for their practice -- and having already started.

---

## The Story (5 min): "The 2 AM Problem"

> "Last spring, I had a problem. My practice was drowning in the same parent questions over and over -- 'Is this rash an emergency?' 'When does a fever need the ER?' 'My kid fell and hit their head, what do I watch for?' I'd answer the same questions at 2 AM, on weekends, in MyChart messages piling up by Monday morning.
>
> I asked our IT department if we could build a simple triage guide. They said they'd get back to me. Three months later, I got an estimate: $40,000 and 6-8 months.
>
> That night, I sat down with Claude and said: 'Help me build a web page where parents can select their child's symptom and get evidence-based guidance on whether to call me, go to urgent care, or go to the ER.' Forty-five minutes later, I had a working prototype.
>
> That was the moment something shifted. Not just 'AI can write code.' The realization that the bottleneck in healthcare isn't ideas. Every single one of you has had an idea for a tool, a workflow, a patient resource, a calculator, a decision aid that would make your practice better. The bottleneck has always been that you needed someone else to build it.
>
> What if that bottleneck just... disappeared?
>
> I'm a pediatrician. I failed the one computer science class I attempted in college. And in the past year, I've built a 33-page educational website, a companion web app with a chatbot, an RSS news feed, interactive tools, and a glossary database. Every line of code was written in collaboration with AI.
>
> Tonight, I'm going to show you exactly how. And then you're going to build something yourself."

---

## The Live Demo (15 min): "The Audience Build"

The audience picks what gets built. This is what makes it unrehearsable and authentic.

### Step 1: The Audience Vote (2 min)

Graham runs a live poll. Three options:

**Option A:** "Antibiotic Duration Calculator" -- Enter infection type and patient factors, get evidence-based duration guidance with printable summary.

**Option B:** "On-Call Handoff Generator" -- Paste patient list notes, get clean SBAR-format handoff document with active issues highlighted.

**Option C:** "Patient Discharge Instruction Builder" -- Select diagnosis, reading level, and language, get customized plain-language discharge instructions.

*(Dr. Hobbs has quietly rehearsed all three but the build is genuinely live.)*

### Step 2: The Build (10-12 min)

Michael builds the winning tool in Claude Code, screen shared, narrating every step.

**Minutes 0-2: The First Prompt**

> "Build me a single-page web app. It should be a Patient Discharge Instruction Builder. The user selects a diagnosis from a dropdown, picks a reading level, and clicks Generate. The app should produce clear, patient-friendly aftercare instructions. Use HTML, CSS, and vanilla JavaScript."

**Commentary:** "Notice I didn't say 'use React' or 'use Tailwind.' I described what I wanted the way I'd describe it to a colleague. That's the entire skill."

**Minutes 2-5: First Result**

The tool appears. Michael clicks around. "It works. But the concussion instructions are generic. And there's no print button."

He iterates: "Add a Print button. Make the concussion instructions more specific -- include return-to-play guidelines and red flag symptoms."

**Minutes 5-8: A Real Failure**

Something breaks. This is THE most important moment.

> "See that? It just broke the dropdown. This happens. Watch what I do -- I don't panic, I don't try to read the code. I just describe the problem."

He types: "The dropdown stopped working after your last change. Fix this and keep the previous improvements."

It recovers. The audience sees the recovery.

**Commentary:** "This right here is the actual skill. Not the first prompt. The iteration. Describing problems clearly. Every single page on AI 101 went through dozens of these cycles."

**Minutes 8-11: The Audience Feature**

Graham reads from the chat: "Someone is asking -- can you add Spanish?"

Michael types: "Add a language toggle that lets the user switch between English and Spanish."

It works. The audience suggestion materialized in real time.

> "That just happened. An audience member had an idea, and it exists now. It took less time than it would take to submit an IT ticket."

**Minutes 11-12: The Reveal**

The tool is live. It exists. It was nothing 12 minutes ago.

> "This isn't production-ready. I wouldn't hand this to patients tomorrow. But I now have a working prototype that would have taken weeks to spec, months to build, and thousands of dollars."

---

## The Lab (10 min): "Your First Build"

Every attendee builds a working tool using Claude.ai Artifacts.

### The Builder's Prompt Template

A fill-in-the-blank prompt shared via chat and displayed on screen:

```
Build me a single-page web tool that does the following:

PURPOSE: [What does this tool do? Describe it like you'd describe it to a colleague.]

THE USER: [Who uses this? A physician? A nurse? A patient? An admin?]

INPUTS: The user should be able to enter or select:
- [Input 1: e.g., "patient age"]
- [Input 2: e.g., "diagnosis from a dropdown"]
- [Input 3: e.g., "insurance type"]

OUTPUT: When they click the button, it should:
[Describe what happens. A calculation? A formatted document? A recommendation?]

DESIGN: Keep it clean, professional, and simple. Use a blue and white color scheme.
```

### The Exercise

1. **Minutes 1-3:** Fill in the template. Graham and Michael show 2-3 examples:
   - "A BMI percentile calculator for pediatric patients"
   - "A prior authorization letter generator"
   - "A shift schedule visualizer"

2. **Minutes 3-8:** Paste into Claude.ai. Artifacts renders a working preview. Click around. Iterate.

3. **Minutes 8-10:** Chat fills with screenshots. Graham narrates:
   > "Dr. Patel just built an insulin dose calculator. Dr. Kim made a consent form generator. Someone just made a tool that converts trial results into patient-friendly summaries."
   >
   > "We're watching 80 physicians build working software simultaneously. None of them have written a line of code. This is what the future looks like."

### Exclusive Content

The Builder's Prompt Template and a curated **"25 Tools Physicians Can Build This Weekend"** list (organized by specialty) are ONLY available to live attendees.

---

## The Framework (10 min): "The Builder's Ladder"

A 4-rung mental model that maps to the series arc:

```
RUNG 1: CONSUMER
"I use tools others built for me"
Skills: Prompting, evaluation, choosing tools
Tools: ChatGPT, Claude, ambient scribes

    ↓

RUNG 2: CUSTOMIZER
"I modify and configure tools for my needs"
Skills: Custom instructions, GPTs, workflows
Tools: Claude Projects, Custom GPTs, Perplexity Spaces

    ↓

RUNG 3: PROTOTYPER
"I build rough tools to test ideas"
Skills: Vibe coding, prompt templates, iteration
Tools: Claude Artifacts, Replit, Lovable

    ↓

RUNG 4: BUILDER
"I create and maintain tools that serve my practice"
Skills: Iteration cycles, deployment, feedback loops
Tools: Claude Code, GitHub, Replit deployment
```

**Key messages:**

- "You don't need to reach Rung 4." Most physicians get enormous value from Rung 2 or 3. The point is the option exists.
- "The ladder is not about code. It's about agency." Every rung represents more control over your own tools.
- "This is the new medical literacy." 20 years ago it meant learning to use an EHR someone else designed. Today it means being able to shape the tools themselves.

### The One Question

> **"What would I build if building were free?"**

"Don't answer it now. Let it percolate. Because the answer is almost certainly something you've already thought of. And now you know how to start."

---

## Pre-Session Materials

1. **Account setup:** Create a free claude.ai account
2. **Homework:** "Think of one small annoyance in your daily workflow. Something repetitive. Write it in one sentence. Bring it to Session 4."
3. **Optional preview:** Link to vibe-coding page on ai101.health
4. **Teaser:** "Dr. Hobbs is going to build a clinical tool live, from scratch. Then you're going to build one too."

## Post-Session Materials

1. Session recording
2. **Builder's Prompt Template**
3. **"25 Tools Physicians Can Build This Weekend"** -- curated list by specialty with starter prompts:
   - Emergency: Laceration repair decision aid, shift swap scheduler
   - Primary Care: Preventive care checklist generator, med interaction lookup
   - Hospitalist: SBAR handoff formatter, progress note template builder
   - Surgery: Post-op instruction generator, timeout checklist tool
   - Psychiatry: PHQ-9/GAD-7 scoring with trend visualization
   - Pediatrics: Growth chart plotter, vaccine catch-up scheduler
   - Practice Management: Prior auth letter generator, patient satisfaction survey builder
4. **Builder's Ladder graphic** (printable)
5. Resource links: ai101.health/vibe-coding, Claude.ai, Replit, Lovable.dev
6. Community info for staying connected

---

## The Wow Moment

**Primary:** The audience feature. Graham reads a suggestion from chat. Michael implements it in 30 seconds. The gap between "having an idea" and "the idea existing" collapses to nothing. The IT ticket line lands.

**Secondary:** The Lab Cascade. Chat fills with screenshots of working tools built by physicians with no coding experience. Graham narrates it in real time. The cascade IS the proof.

---

## Closing Moment

> "Four sessions ago, some of you had never used an AI tool. Tonight you built working software. That arc -- from skeptic to builder in four sessions -- is not about AI. It's about you. It's about physicians reclaiming agency over the tools we use.
>
> For twenty years, technology has been done TO us -- EHRs we didn't design, workflows we didn't choose, interfaces that made our lives harder. AI is the first technology in my career where the power dynamic can flip.
>
> You don't need permission. You don't need a budget. You don't need a computer science degree. You need an idea and the willingness to type 'build me...' and hit enter.
>
> Go build something."

---

---

# Build Requirements

## What Needs to Be Built

### Session 1 Builds

| Item | Description | Effort |
|------|-------------|--------|
| **Lab Web App** | `ai101.health/lab/first-shift` -- scenario selector, prompt input, Claude API backend, CRAFT scoring, live aggregate dashboard | 2-3 days |
| **CRAFT One-Pager** | Printable PDF, co-branded AI 101 + Offcall | 2-3 hours |
| **Pre-Session Poll** | 5 questions, auto-summarize for live reference | 30 min |
| **Aggregate Dashboard** | Presenter-facing view of live CRAFT scores | Built into Lab app |
| **Pre-Session Video** | 90-second phone-recorded casual intro | 1 hour |
| **"Your First AI Week" Email Drip** | 5 emails, Mon-Fri post-session | 2 hours |
| **Prompt Templates Doc** | Exact prompts from demo, fill-in-the-blank format | 1 hour |

### Session 2 Builds

| Item | Description | Effort |
|------|-------------|--------|
| **CRAFT Scorecard PDF** | Laminated-card format, space for 4 tool comparisons | 2-3 hours |
| **Personal Toolkit Planner** | One-page worksheet (Daily Driver, Clinical Partner, Specialist) | 1 hour |
| **Live Polling Setup** | Slido/Mentimeter, 6+ polls, branded | 1-2 hours |
| **Tool Landscape Visual** | Map of AI tools organized by TASK, not company | 3-4 hours |
| **Demo Environment** | 5 tools pre-tested, backup scenario ready | 2-3 hours |

### Session 3 Builds

| Item | Description | Effort |
|------|-------------|--------|
| **Risk Rounds Lab Tool** | `ai101.health/lab/session3` -- 3 scenarios, 4-quadrant input, live aggregation | 1-2 days |
| **SHIELD PDF** | Printable checklist for committee meetings, co-branded | 2-4 hours |
| **Conversation Starter Template** | One-page document for forwarding to CMO/department chair | 2-3 hours |
| **Pre-Tested Demo Vignettes** | 3-4 hallucination triggers, 2-3 bias-revealing pairs, 1-2 privacy scripts. Each with backup recordings. | 4-6 hours |
| **Session 3 Landing Page** | `ai101.health/offcall/session3` resource hub | 2-3 hours |

### Session 4 Builds

| Item | Description | Effort |
|------|-------------|--------|
| **Builder's Prompt Template** | Fill-in-the-blank, tested across multiple use cases | 2-3 hours |
| **Three Rehearsed Demos** | Each option built 2-3 times, failure modes identified | 1-2 days |
| **"25 Tools" Document** | Specialty-organized project ideas with starter prompts | 3-4 hours |
| **Builder's Ladder Graphic** | Clean visual matching AI 101 design language | 2-3 hours |

### Cross-Session Builds

| Item | Description | Effort |
|------|-------------|--------|
| **Series Landing Page** | `ai101.health/offcall` with all 4 sessions, resources, registration | 1 day |
| **Co-Branded Design Templates** | Headers, footers, color scheme for all PDFs and materials | 3-4 hours |
| **Post-Session Email Templates** | Consistent format across all 4 sessions | 2-3 hours |

---

## Risk Mitigation

### Live Demo Failures

A failure during a live demo is a **feature**, not a bug. If AI produces something wrong, it reinforces the session's thesis. However:

- Have screenshots of successful outputs as backup
- Have pre-recorded screen captures tested 24 hours before
- If Claude Code goes down in Session 4, switch to Claude.ai Artifacts

### Lab Technical Issues

- **Fallback:** Attendees open ChatGPT or Claude directly. Graham drops the vignette in chat. They copy-paste and self-score.
- Less elegant, but the exercise still works.

### Audience Skew

- Sessions designed to work at both beginner and advanced levels
- Beginners get the linear learning path
- Advanced users get the frameworks (which most haven't formalized) and specific tool tricks
- Graham calibrates Q&A by calling on a mix

### Privacy and Compliance

- All clinical vignettes are fictional composites
- No real patient data entered into any AI tool during sessions
- Stated explicitly at the start of each session
- Lab apps display disclaimers

### CME Considerations

If seeking CME credit, learning objectives map cleanly:

**Session 1:**
1. Demonstrate use of AI tools for clinical documentation
2. Apply CRAFT framework to evaluate AI-generated clinical content
3. Identify common failure modes of AI-generated medical text

**Session 2:**
1. Compare AI tools for clinical use using structured evaluation criteria
2. Develop a personal AI toolkit matched to clinical workflow
3. Assess AI tool fitness for specific clinical tasks

**Session 3:**
1. Identify hallucination, bias, and privacy risks in clinical AI
2. Apply the SHIELD framework to evaluate AI deployment proposals
3. Articulate AI governance needs to institutional leadership

**Session 4:**
1. Use AI-assisted development to prototype clinical tools
2. Apply iterative prompt refinement to build functional software
3. Evaluate the role of physician-driven tool creation in healthcare

---

## What Exists on AI 101 (Leverage, Don't Rebuild)

| AI 101 Resource | Relevant Sessions |
|-----------------|-------------------|
| start-here.html (NotebookLM on-ramp) | Session 1 |
| prompting.html (The Art of the Ask) | Sessions 1, 4 |
| llm-thinking.html (How LLMs Think Like Clinicians) | Session 1 |
| big-three.html (ChatGPT, Claude, Gemini comparison) | Session 2 |
| clinical-decision-support.html | Session 2 |
| ai-search.html (AI-Powered Search) | Session 2 |
| ambient-ai.html (AI Scribes) | Session 2 |
| bias-ethics.html (Bias, Ethics, Training Data) | Session 3 |
| phi-hipaa.html (Privacy & HIPAA) | Session 3 |
| local-models.html (Running AI Locally) | Session 3 |
| patients-ai.html (When Patients Use AI) | Session 3 |
| environmental-footprint.html | Session 3 |
| vibe-coding.html | Session 4 |
| everyday-ai.html | Session 4 |
| Spark companion app (Tool Finder, Glossary, Prompt Library) | Sessions 1, 2 |

---

## Series Success Metrics

| Metric | Target |
|--------|--------|
| Live attendance per session | 100-200 physicians |
| Lab completion rate | 70%+ of attendees |
| Post-session resource downloads | 50%+ of attendees |
| "Would recommend to colleague" (post-session poll) | 80%+ |
| Return rate (Session 1 → Session 2) | 60%+ |
| Series completion (all 4 sessions) | 40%+ |
| ai101.health traffic increase | 3-5x during series |
| Tool adoption (self-reported in follow-up) | 50%+ using AI weekly within 30 days |

---

*Document prepared for AI 101 x Offcall partnership planning.*
*Content designed by Dr. Michael Hobbs with AI collaboration.*
*February 2026*
