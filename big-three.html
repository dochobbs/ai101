<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>The Big Three — ChatGPT, Claude, and Gemini | AI 101</title>
  <link rel="icon" type="image/png" href="favicon.png">
  <link rel="stylesheet" href="styles.css">
  <script src="https://unpkg.com/lucide@latest"></script>
</head>
<body>

  <nav class="nav">
    <div class="nav-inner">
      <a href="index.html" class="nav-brand">
        <span class="nav-badge">AI 101</span>
        <span class="nav-title">A Self-Paced Guide to AI in Medicine</span>
      </a>
      <button class="nav-toggle" aria-label="Toggle menu">
        <i data-lucide="menu"></i>
      </button>
      <div class="nav-links">
        <a href="index.html" class="nav-link">Topics</a>
        <!-- <a href="resources.html" class="nav-link">Resources</a> -->
        <a href="about.html" class="nav-link">About</a>
        <a href="contribute.html" class="nav-link">Contribute</a>
        <div id="deployment-eac8a421-3c54-4717-bbef-54bb81b54243" class="nav-chatbot"></div>
      </div>
    </div>
  </nav>

  <main class="main">
    <article class="content content-wide">

      <header class="unit-header">
        <span class="unit-label phase-1">FOUNDATIONS</span>
        <h1 class="unit-title">The Big Three</h1>
        <p class="unit-subtitle">
          ChatGPT, Claude, and Gemini—your guide to today's leading foundation
          models and how to choose between them.
        </p>
        <div class="unit-meta">
          <span class="unit-meta-item">
            <i data-lucide="clock"></i>
            ~30 min read
          </span>
          <span class="unit-meta-item">
            <i data-lucide="zap"></i>
            Practical guide
          </span>
        </div>
      </header>

      <div class="callout callout-question">
        <div class="callout-title">Core Question</div>
        <p class="mb-0">
          Which foundation model should you use—and does it actually matter?
        </p>
      </div>

      <!-- Starting Here Section -->
      <h2>Starting Here? Read This First</h2>

      <p>
        If you've jumped straight to this module hoping to pick a model and get started,
        you're in good company—this is exactly what most people want. But foundation models
        are tools, and like any tool, their value depends on how skillfully you use them.
        Before you dive too deep here, consider at least skimming these foundational concepts
        from earlier modules:
      </p>

      <div class="callout callout-info">
        <div class="callout-title">Key Concepts from Earlier Modules</div>
        <p>
          <strong>From Module 1 (How LLMs Think):</strong> These models work by predicting
          the most likely next word in a sequence, trained on enormous datasets of human-generated
          text. They don't "know" things the way you do—they recognize patterns. This matters
          because it explains both their remarkable capabilities and their characteristic failures.
        </p>
        <p>
          <strong>From Module 2 (PHI and HIPAA):</strong> None of these consumer-facing chat
          interfaces are HIPAA-compliant out of the box. We'll discuss BAA pathways later in
          this module, but the critical principle remains: never enter PHI into a consumer AI
          product without proper safeguards.
        </p>
        <p class="mb-0">
          <strong>From Module 3 (Prompting):</strong> The quality of your output depends
          enormously on the quality of your input. A vague prompt produces vague results.
          A well-structured prompt with context, role, and constraints produces dramatically
          better outputs. We'll reference prompting principles throughout this module—they
          apply equally to all three platforms.
        </p>
      </div>

      <p>Now, let's meet the models.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>

      <hr>

      <!-- The Med Student Analogy -->
      <h2>The Med Student Analogy</h2>

      <p>
        Think of each foundation model as a brilliant medical student who has read essentially
        everything ever published—every textbook, every journal article, every clinical guideline,
        every case study, and frankly, every Reddit thread and random blog post too. This student
        has near-perfect recall of patterns across all that material and can synthesize information
        across domains in ways that would take you hours or days.
      </p>

      <p>
        But here's what's crucial: <strong>this med student has never actually seen a patient</strong>.
        They haven't felt the resistance of tissue, watched a parent's face crumple at difficult news,
        or learned from the case that didn't follow the textbook. They know what clinical reasoning
        looks like on paper, but they don't have clinical judgment.
      </p>

      <p>This framing helps calibrate expectations:</p>

      <ul>
        <li>
          <strong>They're genuinely useful</strong> for the kind of work where pattern recognition
          and information synthesis matter—drafting notes, summarizing literature, explaining
          concepts, generating differential diagnoses for discussion.
        </li>
        <li>
          <strong>They require supervision</strong> the same way any trainee does. You wouldn't
          let even a brilliant third-year student sign notes unsupervised, and you shouldn't
          let an AI do so either.
        </li>
        <li>
          <strong>They need clear instructions</strong>. Just as you'd give a student specific
          guidance ("I need a note that addresses the parent's concern about developmental delay,
          focuses on what we observed today, and includes our reasoning for not ordering imaging"),
          you need to give the model explicit context and constraints.
        </li>
        <li>
          <strong>They sometimes confabulate</strong>. A student who doesn't know something might
          make up an answer that sounds plausible rather than saying "I don't know." These models
          do the same—we call it "hallucination," but it's really just pattern-matching in the
          absence of actual knowledge. This is why you verify before you trust.
        </li>
      </ul>

      <p>
        With that framing in mind, let's look at who these three "students" are and what each
        brings to your practice.
      </p>

      <hr>

      <!-- Market Context -->
      <h2>Knowing Your Options</h2>

      <p>
        Before diving into each model, it's worth understanding the landscape:
      </p>

      <ul>
        <li>
          <strong>ChatGPT is the household name.</strong> For most people, "ChatGPT" is synonymous
          with AI chat. It has the largest consumer user base and the most cultural awareness. When
          your patients or colleagues mention "AI," they're usually thinking of ChatGPT.
        </li>
        <li>
          <strong>Many people don't realize they already have Gemini.</strong> If you have a Google
          account, you have access to Gemini. It's built into Google Search, available at gemini.google.com,
          and integrated throughout Google Workspace. Yet many users have never tried it.
        </li>
        <li>
          <strong>Claude is often discovered second.</strong> People typically find Claude when
          looking for alternatives, exploring options for specific use cases, or hearing recommendations
          from colleagues. It has a smaller but dedicated user base.
        </li>
      </ul>

      <p>
        None of this tells you which is "best"—that depends entirely on your needs, your ecosystem,
        and your preferences. The point is simply: you have options, and it's worth knowing what
        they are.
      </p>

      <hr>

      <!-- ChatGPT Section -->
      <h2>ChatGPT: The First Mover</h2>

      <h3>The Story</h3>
      <p>
        OpenAI was founded in December 2015 by Sam Altman, Elon Musk, and others with the stated
        mission of developing artificial general intelligence that benefits humanity. The company
        initially operated as a nonprofit research lab before restructuring in 2019 to attract
        the investment needed for increasingly expensive AI training.
      </p>
      <p>
        The GPT (Generative Pre-trained Transformer) architecture emerged from this research,
        with GPT-1 in 2018, GPT-2 in 2019 (initially withheld due to concerns about misuse),
        and GPT-3 in 2020. But the inflection point came on November 30, 2022, when OpenAI
        released ChatGPT as a free research preview. Within five days, it had a million users.
        Within two months, it reached 100 million—the fastest-growing consumer application in history.
      </p>
      <p>
        That explosive growth fundamentally changed how the world understood AI. Suddenly, anyone
        could have a conversation with a system that felt like talking to a knowledgeable colleague.
        The technology wasn't new, but the accessibility was.
      </p>

      <h3>The Current Offering</h3>
      <p>As of late 2025, ChatGPT operates across several tiers:</p>

      <div class="tier-grid">
        <div class="tier-card">
          <h4>ChatGPT Free</h4>
          <p class="tier-price">$0</p>
          <p>Access to GPT-4o with usage limits. Good for exploration and occasional use.</p>
        </div>
        <div class="tier-card">
          <h4>ChatGPT Plus</h4>
          <p class="tier-price">$20/month</p>
          <p>Higher limits, priority access, o1 reasoning models, DALL-E, advanced voice mode.</p>
        </div>
        <div class="tier-card">
          <h4>ChatGPT Pro</h4>
          <p class="tier-price">$200/month</p>
          <p>For power users. o1 pro mode, extended features, essentially unlimited usage.</p>
        </div>
        <div class="tier-card">
          <h4>ChatGPT Team</h4>
          <p class="tier-price">$25-30/user/month</p>
          <p>Collaborative workspace. Data not used for training. Still not HIPAA-compliant without additional measures.</p>
        </div>
      </div>

      <h3>What You Get</h3>
      <p>
        <strong>Ecosystem:</strong> The most mature AI ecosystem. The GPT Store contains customized
        applications for specific use cases, extensive plugin support, and integrations with tools
        many people already use. If you want to find a pre-built solution for a specific task,
        ChatGPT's ecosystem is the most likely place to find it.
      </p>
      <p>
        <strong>Features:</strong> Advanced Voice Mode for natural conversation, image generation
        through DALL-E, Code Interpreter for data analysis and visualization, and web browsing
        for current information.
      </p>
      <p>
        <strong>HIPAA pathway:</strong> BAAs available for API services and Enterprise/Edu plans
        with sales-managed accounts. ChatGPT Free, Plus, Pro, and Team plans are explicitly not
        covered by BAAs and cannot be used with protected health information.</p>

      <hr>

      <!-- Claude Section -->
      <h2>Claude: The Safety-First Approach</h2>

      <h3>The Story</h3>
      <p>
        Anthropic was founded in 2021 by Dario and Daniela Amodei, along with several other
        former OpenAI researchers and executives. The founding team included key figures in AI
        safety research, and that orientation shaped the company's approach from the beginning.
      </p>
      <p>
        The company developed "Constitutional AI," a training approach that uses AI feedback
        (rather than exclusively human feedback) to shape model behavior according to a set of
        principles. The goal was to create systems that are helpful but also harmless and
        honest—what Anthropic describes as the "HHH" framework.
      </p>
      <p>
        Claude 1.0 launched in March 2023, positioning itself as a thoughtful alternative to
        ChatGPT. The Claude 3 family arrived in March 2024, introducing the Haiku/Sonnet/Opus
        tiering (small/medium/large models with different capability and cost profiles). By late
        2025, Claude Opus 4.5 emerged as the latest iteration, with Anthropic positioning it as
        "the best model in the world for coding, agents, and computer use."
      </p>

      <h3>The Current Offering</h3>

      <div class="tier-grid">
        <div class="tier-card">
          <h4>Claude Free</h4>
          <p class="tier-price">$0</p>
          <p>Access to Claude with usage limits that reset every few hours. Good for exploration.</p>
        </div>
        <div class="tier-card">
          <h4>Claude Pro</h4>
          <p class="tier-price">$20/month</p>
          <p>~5x usage of free tier, all models including Opus, priority access to new features.</p>
        </div>
        <div class="tier-card">
          <h4>Claude Max</h4>
          <p class="tier-price">$100-200/month</p>
          <p>5x-20x Pro limits. Designed for power users, especially Claude Code developers.</p>
        </div>
        <div class="tier-card">
          <h4>Claude Team</h4>
          <p class="tier-price">$25-30/user/month</p>
          <p>Collaborative features, admin controls. Data not used for training. Min 5 seats.</p>
        </div>
      </div>

      <h3>What You Get</h3>
      <p>
        <strong>Ecosystem:</strong> Smaller than ChatGPT's but growing. The MCP (Model Context Protocol)
        allows integrations with external tools and data sources. Claude Code provides command-line
        AI assistance for developers. The ecosystem emphasizes depth over breadth.
      </p>
      <p>
        <strong>Features:</strong> 200,000-token context window (roughly 150,000 words) for working
        with substantial documents. Projects feature for organizing related conversations. Artifacts
        for generating code, documents, and other outputs. No native image generation, but can
        analyze images you provide.
      </p>
      <p>
        <strong>HIPAA pathway:</strong> BAAs available through the API with Zero Data Retention (ZDR)
        agreement. Consumer chat interface (Claude.ai) is not covered. AWS Bedrock provides the
        most straightforward enterprise pathway.
      </p>

      <hr>

      <!-- Gemini Section -->
      <h2>Gemini: The Integration Play</h2>

      <h3>The Story</h3>
      <p>
        Google's path to Gemini began with earlier AI efforts including LaMDA (the model behind
        the original Bard chatbot) and PaLM (Pathways Language Model). But Google's extensive AI
        research, which predates the current generation by decades, positioned the company as a
        natural major player once the race began.
      </p>
      <p>
        Gemini was announced in December 2023 as Google's response to GPT-4, with the company
        emphasizing its native multimodal training—the model was trained from the beginning on
        text, images, and other modalities together, rather than having capabilities bolted on afterward.
      </p>
      <p>
        Gemini 1.5 arrived in February 2024 with a groundbreaking 1-million-token context window
        (later expanded to 2 million)—dramatically larger than competitors at the time. In November
        2025, Google announced Gemini 3, positioned as "our most intelligent model" with significant
        improvements in reasoning, multimodal understanding, and agent capabilities.
      </p>

      <h3>The Current Offering</h3>

      <div class="tier-grid">
        <div class="tier-card">
          <h4>Gemini Free</h4>
          <p class="tier-price">$0</p>
          <p>Access to Gemini 2.5 Flash, basic features, limited Deep Research access.</p>
        </div>
        <div class="tier-card">
          <h4>Google AI Pro</h4>
          <p class="tier-price">$19.99/month</p>
          <p>Gemini 2.5 Pro, 1M-token context, Deep Research, Workspace integration, 2TB storage.</p>
        </div>
        <div class="tier-card">
          <h4>Google AI Ultra</h4>
          <p class="tier-price">$249.99/month</p>
          <p>Gemini 3 Pro with Deep Think, highest limits, 30TB storage, Veo 3 video generation.</p>
        </div>
      </div>

      <h3>What You Get</h3>
      <p>
        <strong>Ecosystem:</strong> Deep Google Workspace integration. If your organization lives
        in Gmail, Docs, Sheets, and Meet, Gemini works natively within those tools—the side panel
        in Google Docs helps you write, Gemini in Gmail drafts responses and summarizes threads.
        For organizations already committed to Google, this reduces friction dramatically. If you
        don't use Google Workspace, much of this advantage disappears.
      </p>
      <p>
        <strong>Features:</strong> 1-2 million token context window—the largest available—for working
        with enormous documents or entire codebases. Strong multimodal capabilities including video
        analysis. Image generation through Imagen. Gemini Live for voice interaction.
      </p>
      <p>
        <strong>HIPAA pathway:</strong> Google Workspace with Gemini is explicitly HIPAA-eligible.
        Google's HIPAA Included Functionality list covers Gemini in Workspace. Organizations that
        sign Google's Business Associate Amendment through the Admin Console can use these services
        with protected health information. This is currently the most straightforward consumer-tier
        HIPAA pathway.
      </p>

      <hr>

      <!-- Comparison Section -->
      <h2>Side-by-Side Comparison</h2>

      <div class="table-responsive">
        <table class="reference-table">
          <thead>
            <tr>
              <th>Factor</th>
              <th>ChatGPT</th>
              <th>Claude</th>
              <th>Gemini</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Developer</strong></td>
              <td>OpenAI (Microsoft-backed)</td>
              <td>Anthropic (Amazon/Google-backed)</td>
              <td>Google DeepMind</td>
            </tr>
            <tr>
              <td><strong>Consumer Price Entry</strong></td>
              <td>$20/month (Plus)</td>
              <td>$20/month (Pro)</td>
              <td>$19.99/month (AI Pro)</td>
            </tr>
            <tr>
              <td><strong>Premium Tier</strong></td>
              <td>$200/month (Pro)</td>
              <td>$200/month (Max 20x)</td>
              <td>$249.99/month (Ultra)</td>
            </tr>
            <tr>
              <td><strong>Context Window</strong></td>
              <td>128K tokens</td>
              <td>200K tokens</td>
              <td>1-2M tokens</td>
            </tr>
            <tr>
              <td><strong>Native Image Gen</strong></td>
              <td>Yes (DALL-E)</td>
              <td>No</td>
              <td>Yes (Imagen)</td>
            </tr>
            <tr>
              <td><strong>Voice Mode</strong></td>
              <td>Advanced Voice Mode</td>
              <td>Limited</td>
              <td>Gemini Live</td>
            </tr>
            <tr>
              <td><strong>Ecosystem</strong></td>
              <td>GPT Store, plugins</td>
              <td>MCP integrations</td>
              <td>Google Workspace</td>
            </tr>
          </tbody>
        </table>
      </div>

      <h3>HIPAA/BAA Comparison</h3>

      <div class="table-responsive">
        <table class="reference-table">
          <thead>
            <tr>
              <th>Platform</th>
              <th>Consumer Chat BAA</th>
              <th>API BAA</th>
              <th>Easiest Path</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>ChatGPT</strong></td>
              <td>Enterprise/Edu only</td>
              <td>Yes, via application</td>
              <td>Azure OpenAI Service</td>
            </tr>
            <tr>
              <td><strong>Claude</strong></td>
              <td>No</td>
              <td>Yes, with ZDR agreement</td>
              <td>AWS Bedrock</td>
            </tr>
            <tr>
              <td><strong>Gemini</strong></td>
              <td>Workspace integration covered</td>
              <td>Yes via Vertex AI</td>
              <td>Google Workspace + BAA</td>
            </tr>
          </tbody>
        </table>
      </div>

      <div class="callout callout-warning">
        <div class="callout-title">Critical Reminder</div>
        <p class="mb-0">
          For the consumer chat interfaces most people use day-to-day (ChatGPT Plus, Claude Pro,
          standard Gemini), <strong>none are HIPAA-compliant</strong> and should never be used
          with PHI without additional safeguards.
        </p>
      </div>

      <hr>

      <!-- Just Pick One -->
      <h2>Just Pick One and Start</h2>

      <p>
        Here's the honest truth: <strong>for most clinical use cases, all three models are capable
        enough</strong>. The differences between them matter at the margins—and those margins shift
        with every model update anyway.
      </p>

      <p>
        Don't overthink the choice. Pick based on:
      </p>

      <ul>
        <li><strong>What you already have access to.</strong> Already in Google Workspace? Try Gemini. Have a ChatGPT account from when everyone was talking about it? Use that.</li>
        <li><strong>What your colleagues use.</strong> There's value in being able to share prompts and tips with people doing similar work.</li>
        <li><strong>Whichever interface you prefer.</strong> Seriously—if one feels better to use, that matters.</li>
      </ul>

      <p>
        You can always switch later. You can use multiple models for different tasks. The skill you
        develop—learning to prompt effectively, knowing when to trust outputs, building useful
        workflows—transfers across all of them.
      </p>

      <p>
        The next section gives you a practical plan for getting started. After that, we'll cover
        how to evaluate models with your own use cases over time.
      </p>

      <hr>

      <!-- Practical Guidance -->
      <h2>Practical Guidance: Your First Month</h2>

      <h3>Week 1: Establish a Baseline</h3>
      <p>
        Choose whichever model you have easiest access to. For your first week, use it for
        low-stakes tasks:
      </p>
      <ul>
        <li>Summarizing an article you're reading anyway</li>
        <li>Drafting an email you'll heavily edit</li>
        <li>Explaining a concept to yourself before explaining it to a patient</li>
        <li>Brainstorming questions for a meeting</li>
      </ul>
      <p>Don't worry about optimization. Just get comfortable with the interaction pattern.</p>

      <h3>Week 2: Apply Prompting Principles</h3>
      <p>Revisit the prompting framework from Module 3 and apply it deliberately:</p>
      <ul>
        <li>Give the model a role ("You are helping me prepare for a difficult conversation with a parent")</li>
        <li>Provide context ("The patient is 8 years old with a new ADHD diagnosis")</li>
        <li>Be specific about format ("I need three main points, in language a non-medical parent would understand")</li>
        <li>Include constraints ("Avoid medical jargon; emphasize that this is manageable")</li>
      </ul>
      <p>Notice how the quality of outputs changes as you prompt more skillfully.</p>

      <h3>Week 3: Try Something Harder</h3>
      <p>Push into a task that actually matters:</p>
      <ul>
        <li>Draft a real (de-identified) clinical note</li>
        <li>Summarize a complex patient case for a referral</li>
        <li>Create patient education materials for a condition you see frequently</li>
        <li>Analyze a clinical guideline and identify key practice implications</li>
      </ul>
      <p>
        Evaluate the output critically. What did it get right? Where did it need correction?
        What would you prompt differently next time?
      </p>

      <h3>Week 4: Compare</h3>
      <p>
        Now try a second model with a task you've done before. Use the same prompt and compare
        outputs. You'll develop intuition for the differences—and often find that your preference
        is less about the model and more about how you've learned to work with it.
      </p>

      <hr>

      <!-- Evaluate With Your Own Use Cases -->
      <h2>Evaluate With Your Own Use Cases</h2>

      <p>
        Here's a truth that benchmark tables and feature comparisons can't capture: <strong>the only
        evaluation that matters is how a model performs on your actual work</strong>.
      </p>

      <h3>Build Your Personal Test Set</h3>
      <p>
        Create a small set of 3-5 tasks that represent your real work:
      </p>
      <ul>
        <li>A type of document you frequently draft (note, letter, summary)</li>
        <li>A question you commonly need to research or explain</li>
        <li>A complex case or scenario you've worked through before</li>
        <li>Something where you know what "good" looks like</li>
      </ul>
      <p>
        Run these same prompts through different models. Compare the outputs. Which required less
        editing? Which understood your intent better? Which produced something you'd actually use?
      </p>

      <h3>Re-Evaluate After Updates</h3>
      <p>
        This is crucial and often overlooked: <strong>a model that didn't work for you six months ago
        might be excellent now</strong>. And vice versa—a model you loved might change in ways that
        don't suit your workflow.
      </p>
      <p>
        Each major model update (GPT-4 to GPT-4o, Claude 3 to Claude 4, etc.) can significantly change
        how the model handles specific tasks. Some examples:
      </p>
      <ul>
        <li>A clinical note format that one model version struggled with might work perfectly in the next</li>
        <li>A type of analysis that frustrated you might become seamless after an upgrade</li>
        <li>Conversely, a workflow you'd perfected might break when the model changes</li>
      </ul>
      <p>
        When you see announcements about major model updates, revisit your test set. Don't assume
        your current choice is still the best choice—or that a model you dismissed is still inadequate.
      </p>

      <div class="callout callout-principle">
        <div class="callout-title">The Ongoing Evaluation Habit</div>
        <p class="mb-0">
          Set a calendar reminder every 3-6 months to run your personal test set across the current
          versions of each model. This takes 30 minutes and ensures you're always using the best
          tool for your needs—not just the one you happened to start with.
        </p>
      </div>

      <hr>

      <!-- The Cost Question -->
      <h2>The Cost Question</h2>

      <p>Let's be direct about money.</p>

      <p>
        <strong>Free tiers are sufficient for exploration and light use.</strong> If you're using
        AI occasionally—a few times a week for non-critical tasks—you may never need to pay.
      </p>

      <p>
        <strong>$20/month is the standard paid tier.</strong> ChatGPT Plus, Claude Pro, and
        Google AI Pro all cluster around this price point. At this tier, you get higher usage limits,
        access to the best models, and priority features. For a professional tool you might use daily,
        $20/month is modest—less than many software subscriptions with narrower utility. This is
        where most regular users land.
      </p>

      <p>
        <strong>Premium tiers ($200-250/month) are for power users.</strong> If you're running into
        limits on the $20 tier, pushing complex coding projects, or need maximum model capabilities
        for professional work, the premium tiers exist. But most users won't need them.
      </p>

      <div class="callout callout-principle">
        <div class="callout-title">Recommendation for Most Clinicians</div>
        <ol class="mb-0">
          <li>Start with free tiers</li>
          <li>Upgrade to ~$20/month when you hit limits regularly</li>
          <li>Evaluate premium tiers only if you're a genuine power user</li>
          <li>Discuss organizational deployment with compliance and IT before rolling out team solutions</li>
        </ol>
      </div>

      <hr>

      <!-- Benchmarks Section -->
      <h2>Understanding AI Benchmarks</h2>

      <div class="callout callout-warning">
        <div class="callout-title">Benchmarks Have Limited Value</div>
        <p class="mb-0">
          This section explains benchmarks because you'll encounter them in AI discussions. But
          here's the key point upfront: benchmark scores tell you very little about how useful a
          model will be for your specific work. A 2% difference on a test doesn't translate to
          a meaningfully better tool for writing clinical notes or explaining diagnoses. Read this
          section for context, then focus on your own evaluation.
        </p>
      </div>

      <p>
        You'll often see AI companies touting benchmark scores when announcing new models.
        Headlines declare one model "beats" another on some test. But what do these numbers
        actually mean—and more importantly, what <em>don't</em> they mean?
      </p>

      <h3>What Benchmarks Measure</h3>
      <p>
        Benchmarks are standardized tests designed to evaluate specific AI capabilities.
        They provide a common yardstick for comparing models on tasks like:
      </p>
      <ul>
        <li><strong>Knowledge recall:</strong> Can the model answer questions across academic domains?</li>
        <li><strong>Reasoning:</strong> Can it work through multi-step logic problems?</li>
        <li><strong>Coding:</strong> Can it write functional code that solves programming challenges?</li>
        <li><strong>Domain expertise:</strong> How does it perform on professional exams (medical, legal, etc.)?</li>
      </ul>

      <h3>What Benchmarks Don't Measure</h3>
      <p>
        Here's what no benchmark captures—and what often matters most for real-world use:
      </p>
      <ul>
        <li><strong>Writing quality:</strong> Does the output sound natural and require minimal editing?</li>
        <li><strong>Appropriate uncertainty:</strong> Does the model say "I don't know" when it should?</li>
        <li><strong>Instruction following:</strong> Does it do what you actually asked, or what it thinks you asked?</li>
        <li><strong>Consistency:</strong> Does it give similar quality outputs across multiple attempts?</li>
        <li><strong>Your specific use case:</strong> A model that aces coding benchmarks might produce worse clinical notes than one that scores lower.</li>
      </ul>

      <div class="callout callout-warning">
        <div class="callout-title">The Gap Between Benchmarks and Reality</div>
        <p class="mb-0">
          A model scoring 90% vs 88% on a knowledge test is not meaningfully different for
          most practical purposes. What matters is whether the model helps <em>you</em> do
          <em>your</em> work better. The only benchmark that truly matters is your own
          experience using the tool.
        </p>
      </div>

      <h3>How to Use Benchmark Data</h3>
      <ol>
        <li><strong>As a rough filter:</strong> If a model scores dramatically lower on everything, it's probably less capable overall.</li>
        <li><strong>For specific tasks:</strong> If you primarily need coding help, look at coding benchmarks. For medical questions, look at MedQA scores.</li>
        <li><strong>With skepticism:</strong> Companies often cherry-pick favorable benchmarks. Independent testing (like LMArena) provides more balanced pictures.</li>
        <li><strong>As a starting point:</strong> Use benchmarks to decide which 2-3 models to try, then let your own experience guide your choice.</li>
      </ol>

      <h3>Key Benchmarks Explained</h3>

      <div class="table-responsive">
        <table class="reference-table">
          <thead>
            <tr>
              <th>Benchmark</th>
              <th>What It Tests</th>
              <th>Why It Matters</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Humanity's Last Exam</strong></td>
              <td>2,500 expert-level questions across all disciplines, designed to be genuinely difficult</td>
              <td>The hardest general benchmark; tests limits of AI reasoning</td>
            </tr>
            <tr>
              <td><strong>GPQA Diamond</strong></td>
              <td>PhD-level science questions (physics, chemistry, biology)</td>
              <td>Deep reasoning in scientific domains</td>
            </tr>
            <tr>
              <td><strong>MedQA</strong></td>
              <td>USMLE-style medical licensing questions</td>
              <td>Medical knowledge directly relevant to clinical practice</td>
            </tr>
            <tr>
              <td><strong>SimpleQA</strong></td>
              <td>Short fact-seeking questions with single correct answers</td>
              <td>Measures hallucination rate and factual accuracy</td>
            </tr>
            <tr>
              <td><strong>SimpleBench</strong></td>
              <td>Common-sense reasoning that humans find easy but AI finds hard</td>
              <td>Tests practical reasoning vs pattern matching</td>
            </tr>
            <tr>
              <td><strong>LMArena Elo</strong></td>
              <td>Human preference ratings from blind head-to-head comparisons</td>
              <td>Real users choosing preferred outputs—closest to "usefulness"</td>
            </tr>
            <tr>
              <td><strong>SWE-Bench</strong></td>
              <td>Fixing real bugs in actual open-source codebases</td>
              <td>Real-world software engineering capability</td>
            </tr>
          </tbody>
        </table>
      </div>

      <h3>Benchmark Results (A Snapshot in Time)</h3>
      <p>
        Below are scores for flagship models as of late 2025. <strong>These numbers will be outdated
        quickly</strong>—new model versions release every few months, and the leaderboard constantly
        shifts. We include them to illustrate the general landscape, not to make a definitive ranking.
      </p>

      <div class="table-responsive">
        <table class="reference-table benchmark-table">
          <thead>
            <tr>
              <th>Benchmark</th>
              <th>GPT-5.1</th>
              <th>Claude Opus 4.5</th>
              <th>Gemini 3 Pro</th>
              <th>Notes</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Humanity's Last Exam</strong></td>
              <td>~36%</td>
              <td>~35%</td>
              <td>45.8%</td>
              <td>Gemini leads on hardest reasoning test</td>
            </tr>
            <tr>
              <td><strong>GPQA Diamond</strong></td>
              <td>88.1%</td>
              <td>87.0%</td>
              <td>91.9%</td>
              <td>Gemini ahead on PhD-level science</td>
            </tr>
            <tr>
              <td><strong>MedQA</strong></td>
              <td>~96%</td>
              <td>~94%</td>
              <td>~93%</td>
              <td>All far exceed passing threshold (~60%)</td>
            </tr>
            <tr>
              <td><strong>SimpleQA (Factuality)</strong></td>
              <td>~63%</td>
              <td>~45%</td>
              <td>72.1%</td>
              <td>Gemini leads on factual accuracy</td>
            </tr>
            <tr>
              <td><strong>LMArena Elo</strong></td>
              <td>~1480</td>
              <td>~1470</td>
              <td>1501</td>
              <td>Gemini tops human preference ratings</td>
            </tr>
            <tr>
              <td><strong>SWE-Bench Verified</strong></td>
              <td>76.3%</td>
              <td>80.9%</td>
              <td>76.2%</td>
              <td>Claude leads on real-world coding</td>
            </tr>
          </tbody>
        </table>
      </div>

      <div class="callout callout-info">
        <div class="callout-title">The Bottom Line on Benchmarks</div>
        <p class="mb-0">
          The numbers in this table will shift with every model release. What matters is this:
          <strong>all three models score 93%+ on MedQA</strong>—far above the passing threshold
          for medical licensing exams. For the vast majority of clinical use cases, all three are
          capable enough. Don't choose based on benchmark margins. Choose based on ecosystem fit,
          pricing, and—most importantly—how well the model actually performs on your specific work.
        </p>
      </div>

      <hr>

      <!-- Common Pitfalls -->
      <h2>Common Pitfalls and How to Avoid Them</h2>

      <div class="concepts-grid">
        <div class="concept-card">
          <div class="concept-icon"><i data-lucide="check-check"></i></div>
          <div class="concept-name">Treating Output as Truth</div>
          <div class="concept-desc">The models generate plausible text, not verified facts. Treat all outputs as first drafts requiring verification.</div>
        </div>
        <div class="concept-card">
          <div class="concept-icon"><i data-lucide="shield-off"></i></div>
          <div class="concept-name">Assuming Privacy</div>
          <div class="concept-desc">Consumer AI tools aren't HIPAA-compliant. Establish clear rules about what can be entered into which tools.</div>
        </div>
        <div class="concept-card">
          <div class="concept-icon"><i data-lucide="trending-down"></i></div>
          <div class="concept-name">Underusing the Tools</div>
          <div class="concept-desc">One bad result doesn't mean AI isn't useful. Commit to sustained experimentation over weeks, not minutes.</div>
        </div>
        <div class="concept-card">
          <div class="concept-icon"><i data-lucide="brain-cog"></i></div>
          <div class="concept-name">Overusing the Tools</div>
          <div class="concept-desc">Don't outsource judgment you should retain. Use AI to augment, not replace, your critical thinking.</div>
        </div>
      </div>

      <hr>

      <!-- Quick Reference -->
      <h2>Quick Reference: Getting Started</h2>

      <div class="quick-ref-grid">
        <div class="quick-ref-card">
          <h4>ChatGPT</h4>
          <p><strong>URL:</strong> chat.openai.com</p>
          <p><strong>Mobile:</strong> iOS and Android apps</p>
          <p><strong>Sign up:</strong> Google, Microsoft, or email</p>
          <p><strong>Best for:</strong> Broad capabilities, images, mature ecosystem</p>
        </div>
        <div class="quick-ref-card">
          <h4>Claude</h4>
          <p><strong>URL:</strong> claude.ai</p>
          <p><strong>Mobile:</strong> iOS and Android apps</p>
          <p><strong>Sign up:</strong> Google, email, or phone</p>
          <p><strong>Best for:</strong> Nuanced writing, analysis, coding, long documents</p>
        </div>
        <div class="quick-ref-card">
          <h4>Gemini</h4>
          <p><strong>URL:</strong> gemini.google.com</p>
          <p><strong>Mobile:</strong> iOS and Android apps</p>
          <p><strong>Sign up:</strong> Google account required</p>
          <p><strong>Best for:</strong> Google Workspace, massive context, HIPAA via Workspace</p>
        </div>
      </div>

      <hr>

      <!-- Action Items -->
      <h2>Action Items</h2>

      <p>Before moving to the next module, complete at least one of these:</p>

      <ol>
        <li>
          <strong>Create accounts on all three platforms</strong> (if you haven't already).
          Even if you end up preferring one, having tried the others gives you useful context.
        </li>
        <li>
          <strong>Run the same prompt through all three</strong> and compare outputs. Notice
          the differences in tone, organization, and approach.
        </li>
        <li>
          <strong>Try something you actually need:</strong> Draft a real email, summarize a
          real article, prepare for a real conversation. Experience how the tool performs on
          your actual work.
        </li>
        <li>
          <strong>Hit a limit and recover:</strong> Deliberately work on something complex
          enough that your first output isn't good. Practice iterative refinement until
          you're satisfied.
        </li>
        <li>
          <strong>If you're in a healthcare organization:</strong> Identify which HIPAA pathway
          makes sense for your situation and discuss it with appropriate stakeholders.
        </li>
      </ol>

      <hr>

      <!-- Summary -->
      <h2>Summary</h2>

      <p>
        ChatGPT, Claude, and Gemini are all capable foundation models that can meaningfully assist
        clinical work. Each has different ecosystem integrations, pricing structures, and HIPAA pathways.
        ChatGPT has the largest user base and most extensive ecosystem. Gemini integrates deeply with
        Google Workspace and offers the most straightforward consumer HIPAA pathway. Claude offers
        the largest context window outside of Gemini and strong developer tools.
      </p>

      <p>
        But here's what matters most: <strong>the choice between them matters far less than developing
        skill with whichever you choose</strong>. These are tools that respond to how you use them.
        A well-crafted prompt to any of these models will outperform a vague prompt to the "best" model.
      </p>

      <p>
        And remember: your evaluation shouldn't be a one-time event. Models improve, workflows change,
        and what didn't work last year might work beautifully now. Build the habit of periodic
        re-evaluation with your own real-world test cases.
      </p>

      <div class="callout callout-principle">
        <div class="callout-title">The Bottom Line</div>
        <p class="mb-0">
          Think of them as brilliant but inexperienced colleagues: genuinely helpful, occasionally
          wrong, always requiring supervision. Pick one based on your ecosystem and access. Use it
          enough to develop skill. Periodically test alternatives with your own use cases. With that
          approach and the prompting skills from earlier modules, you're ready to begin. Now close
          this document and go have a conversation with one of them. That's where the real learning happens.
        </p>
      </div>

      <!-- Learning Objectives -->
      <div class="objectives phase-1">
        <h4 class="objectives-title">Learning Objectives</h4>
        <ul class="objectives-list">
          <li>Compare the capabilities, strengths, and limitations of ChatGPT, Claude, and Gemini</li>
          <li>Identify which model best fits specific clinical use cases</li>
          <li>Understand HIPAA compliance pathways for each platform</li>
          <li>Apply a practical framework for getting started with foundation models</li>
          <li>Recognize common pitfalls in AI adoption and strategies to avoid them</li>
        </ul>
      </div>

      <!-- Footnotes -->
      <hr>
      <section class="footnotes">
        <h4>Notes</h4>
        <ol class="footnote-list">
          <li id="fn1">
            <strong>Why not Grok?</strong> You may wonder why xAI's Grok isn't included here.
            While Grok has some capable underlying technology, we don't recommend it for clinical
            use due to significant concerns about accuracy and safety guardrails.
            <p>
              In early 2025, Grok generated and spread false information about prominent public
              figures, including fabricated claims about an NBA owner that were widely amplified
              on X (formerly Twitter). The system was also found to produce misleading election-related
              content and struggled with basic factual queries where other models performed reliably.
              Independent evaluations have noted that Grok's safety measures are notably weaker than
              those of ChatGPT, Claude, or Gemini—the model is more likely to generate harmful content
              when prompted.
            </p>
            <p>
              For clinical applications where accuracy and appropriate guardrails matter, these
              issues are disqualifying. The three platforms covered in this module have demonstrated
              more robust approaches to safety and factual accuracy—though as we discuss throughout,
              all AI outputs require verification.
            </p>
            <p class="footnote-refs">
              <strong>References:</strong><br>
              Newsweek. "<a href="https://www.newsweek.com/mark-cuban-elon-musk-ai-bot-1944043">Mark Cuban Confronts Elon Musk Using His Own AI Bot</a>." 2025.<br>
              Axios. "<a href="https://www.axios.com/2024/08/05/elon-musk-grok-2024-election-ballot-misinformation">Musk's AI chatbot spread election misinformation, secretaries of state say</a>." August 2024.<br>
              Center for Countering Digital Hate. "<a href="https://counterhate.com/research/grok-ai-election-disinformation/">Grok AI Election Disinformation</a>." 2024.<br>
              Palo Alto Networks Unit 42. "<a href="https://unit42.paloaltonetworks.com/comparing-llm-guardrails-across-genai-platforms/">How Good Are the LLM Guardrails on the Market?</a>." 2024.
            </p>
            <a href="#fnref1" class="footnote-back">↩</a>
          </li>
        </ol>
      </section>

      <!-- Page Navigation -->
      <nav class="page-nav">
        <a href="prompting.html" class="page-nav-link prev">
          <span class="page-nav-arrow"><i data-lucide="arrow-left"></i></span>
          <div>
            <span class="page-nav-label">Previous</span>
            <span class="page-nav-title">The Art of the Ask</span>
          </div>
        </a>
        <a href="bias-ethics.html" class="page-nav-link next">
          <span class="page-nav-arrow"><i data-lucide="arrow-right"></i></span>
          <div>
            <span class="page-nav-label">Next</span>
            <span class="page-nav-title">Bias, Ethics, and the Training Data Problem</span>
          </div>
        </a>
      </nav>

    </article>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      <div><strong>AI 101</strong> · A Self-Paced Guide to AI in Medicine</div>
      <div>v1.1 · 2025</div>
    </div>
  </footer>

  <script>
    lucide.createIcons();
    const navToggle = document.querySelector('.nav-toggle');
    const navLinks = document.querySelector('.nav-links');
    if (navToggle) {
      navToggle.addEventListener('click', () => {
        navLinks.classList.toggle('nav-open');
      });
    }
  </script>

  <script src="https://studio.pickaxe.co/api/embed/bundle.js" defer></script>

</body>
</html>
